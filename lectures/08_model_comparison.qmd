---
title: "Modern Statistical Computing"
subtitle: "8. Model comparison"
author: "David Rossell"
institute: "Pompeu Fabra University"
execute:
  echo: true
format:
  revealjs:
    theme: [default, custom.scss]
    scrollable: true
toc: true
toc-depth: 1
number-sections: false
mouse-wheel: true
code-overflow: scroll
code-line-numbers: false
code-copy: true
cache: false
title-slide-style: pandoc
bibliography: references.bib
---

## Reproducing these lecture notes

Load required R packages

```{r, warning=FALSE}
library(tidyverse)
library(boot)
library(brglm)
library(coefplot)
library(mombf)
library(openintro)
library(pROC)
```

`mombf` requires `sparseMatrixStats`. Install it as follows
```{r, eval=FALSE}
library(BiocManager)
BiocManager::install("sparseMatrixStats")
```


We also source routines.R, which has auxiliary functions

```{r}
source('../code/routines.R')
```


##

Typically we're unsure about what regression model is best. Consider these situations

- We add parameters (e.g. variables) to a model. We wanna test if they're statistically significant *by comparing the 2 models*

- We have $\geq 2$ models. We seek the best one for *forecasting*

- We have $\geq 2$ models. We seek the best one for *explaining*

We discuss methods for each of these situations


# Testing 2 nested models

## Nested models

Let outcome $y \in \mathbb{R}^n$, $X_1$ is $n \times p_1$, $X_2$ is $n \times p_2$

- Model 1 has a regression equation driven by $X_1 \beta_1$, $\beta_1 \in \mathbb{R}^{p_1}$

- Model 2 features $X_1 \beta_1 + X_2 \beta_2$, $\beta_1 \in \mathbb{R}^{p_1}$, $\beta_2 \in \mathbb{R}^{p_2}$

**Example.** If $y_i$ is a Gaussian outcome,

Model 1: $y_i = x_{1i}^T \beta_1 + \epsilon_i$

Model 2: $y_i = x_{1i}^T \beta_1 + x_{2i}^T \beta_2 + \epsilon_i$

**Goal.** Test the null hypothesis $H_0: \beta_2=0$

The standard test for such $H_0$ is the *likelihood ratio test*


## Likelihood ratio test

Let $\theta_1=\beta_1$ be the parameters under Model 1, $\theta_2=(\beta_1,\beta_2)$ those under Model 2

Consider the maximized log-likelihoods
$$
\begin{aligned}
L_1= \max_{\theta_1} \log p(y \mid \theta_1) \\
L_2= \max_{\theta_2} \log p(y \mid \theta_2)
\end{aligned}
$$

**Def.** The *likelihood ratio test statistic* is $\mbox{LR}_{21}= 2(L_2 - L_1)$

Large $\mbox{LR}_{21}$ indicate that data $y$ are more likely under Model 2, i.e. evidence against $H_0:\beta_2=0$

## Likelihood ratio test

Consider 2 nested models and $\mbox{LR}_{21}$ as defined above, **where $H_0:\beta_2=0$ is true**

**Result.** For Normal linear regression,
$$
\mbox{LR}_{21} \sim F_{p_2, n-p_2}
$$
where $F_{a,b}$ denotes an F-distribution with degrees of freedom $(a,b)$

**Result.** For generalized linear models, under mild technical conditions as $n \rightarrow \infty$
$$
\mbox{LR}_{21} \stackrel{D}{\longrightarrow} \chi_{p_2}^2
$$

Note: as $n \rightarrow \infty$, $F_{p_2,n-p_2} \stackrel{D}{\longrightarrow} \chi_{p_2}^2$

## Likelihood ratio test

These results assume that the model is correct. For example, in linear regression, that data truly follow a Normal linear regression where

$$
y = X_1 \beta_1 + \epsilon \mbox{, where } \epsilon \sim N(0, \sigma^2 I)
$$

In R, LRTs are implemented in function `anova`

Otherwise, one can estimate the distribution of $\mbox{LR}_{21}$ under $H_0$ using permutations

## Example

<small> (data from Lander's [R for everyone](https://www.jaredlander.com/r-for-everyone/data-in-r-for-everyone), from NYC Open Data) </small>

$n=2,626$ condos in different New York boroughs (2011-12)

- Outcome: value per square foot (`ValuePerSqFt`)

- 10 covariates: neighborhood, year built etc.



```{r}
housing= read_csv("../datasets/housing.csv")
housing
```

## Outcome distribution

:::panel-tabset

### Outcome

```{r}
#| code-fold: true
ggplot(housing, aes(x=ValuePerSqFt)) +
  geom_density()
```


### Outcome vs borough

```{r}
#| code-fold: true
ggplot(housing, aes(x=ValuePerSqFt)) +
  geom_density(aes(color=Boro))
```

:::


## Transformations


:::panel-tabset

### SqFt

```{r}
#| code-fold: true
ggplot(housing, aes(x=SqFt, y=ValuePerSqFt)) + geom_point() + geom_smooth()
```

### Units

```{r}
#| code-fold: true
ggplot(housing, aes(x=Units, y=ValuePerSqFt)) + geom_point() + geom_smooth()
```

### log(SqFt)

```{r}
#| code-fold: true
ggplot(housing, aes(x=log10(SqFt), y=ValuePerSqFt)) + geom_point() + geom_smooth()
```

### log(Units)

```{r}
#| code-fold: true
ggplot(housing, aes(x=log10(Units), y=ValuePerSqFt)) + geom_point() + geom_smooth()
```

:::


##

Fit 2 models: additive and with interactions

```{r}
housing= transform(housing, lSqFt= log10(SqFt), lUnits= log10(Units))
fit1= lm(ValuePerSqFt ~ Boro + lSqFt + lUnits , data=housing)
fit2= lm(ValuePerSqFt ~ Boro + lSqFt + lUnits + Boro:lSqFt + Boro:lUnits, data=housing)
```

:::panel-tabset

### fit1

```{r}
coefSummary(fit1)
```

### coefPlot1

```{r}
coefplot(fit1)
```

### fit2

```{r}
coefSummary(fit2)
```


### coefPlot2

```{r}
coefplot(fit2)
```

:::

##

Single interaction coef. were not significant. Let's test if **the whole vector is 0**

```{r}
anova(fit1,fit2)  #LRT (=F-test in Gaussian regression)
```

Same when testing each interaction separately

```{r}
fit3= lm(ValuePerSqFt ~ Boro + lSqFt + lUnits + Boro:lSqFt, data=housing)

fit4= lm(ValuePerSqFt ~ Boro + lSqFt + lUnits + Boro:lUnits, data=housing)

anova(fit2, fit3)
anova(fit2, fit4)
```

##

Coef. for boroughs were not significant. But recall their interpretation! If we have

$$
E(y_i) = \beta_0 + \beta_1 \mbox{manhattan}_i + \beta_2 \mbox{lSqFt}_i + \beta_3 \mbox{manhattan}_i \times \mbox{lSqFt}_i
$$
then $\beta_1$ is the difference between Manhattan and the reference borough **when $\mbox{lSqFt}_i=0$**. But all condos have $\mbox{lSqFt}_i > 2.5$

Re-parameterize the model to facilitate interpretation. Let lSqFt0 be a mean-centered version of lSqFt, so that $\mbox{lSqFt0}_i = 0 \Rightarrow \mbox{lSqFt}_i=\mbox{mean(lSqFt)}$

$$
E(y_i) = \beta_0 + \beta_1 \mbox{manhattan}_i + \beta_2 \mbox{lSqFt0}_i + \beta_3 \mbox{manhattan}_i \times \mbox{lSqFt0}_i
$$

Now $\beta_1$ is the Manhattan effect for condos with average log-SqFt

##

In the mean-centered model boroughs now have a significant effect, as expected

```{r}
housing= transform(housing, lSqFt0= lSqFt - mean(lSqFt), lUnits0= lUnits - mean(lUnits))
fit5= lm(ValuePerSqFt ~ Boro + lSqFt0 + lUnits0 + Boro:lSqFt0 + Boro:lUnits0, data=housing)
coefplot(fit5)
```


## Take-home message


Testing $(\beta_1,\beta_2)=(0,0)$ is not the same as testing $\beta_1=0$ and $\beta_2=0$. Even if the latter are both non-significant, the former may be


We must interpret such tests carefully when there's parameter interactions. Mean-centering the covariates may help a bit


# Best model for forecasting

## Forecasting is not testing

Hypothesis tests are conservative in nature, e.g. unless fairly sure that $\beta_1 \neq 0$, set $\beta_1=0$

Estimating and forecasting tend to be more liberal, e.g. MLE $\hat{\beta}_1$ has reasonable predictive properties as $n$ grows. This said, adding unnecessary parameters to a model can hurt predictions

**Over-fitting** is a key notion to assess forecasting accuracy of a model/algorithm


## Example

Simulate $n=10$ observations from
$$
y_i= \sin(2\pi x) + \epsilon_i \mbox{, where } \epsilon_i \sim N(0,0.2^2)
$$

```{r}
#| code-fold: true
x= seq(0,1, length=10)
y= sin(2*pi*x) + rnorm(10,0,sd=0.2)
xseq= seq(0,1,length=500)

## Create design matrix for up to degree 9
xpol= matrix(NA, nrow= length(x), ncol=10)
xseqpol= matrix(NA, nrow= length(xseq), ncol=10)
for (j in 1:10) {
    xpol[,j]= x^(j-1)
    xseqpol[,j]= xseq^(j-1)
}

par(mar=c(4,4,.1,.1), cex.lab=1.3, cex.axis=1.3)
plot(x, y)
lines(xseq, sin(2*pi*xseq))
```


---

Fit polynomials of degree 0, 1, 3 and 9. Which should predict *future data* better?

```{r}
#| code-fold: true
fit0= lm(y ~ 1); b0= coef(fit0)
fit1= lm(y ~ -1 + xpol[,1:2]); b1= coef(fit1)
fit3= lm(y ~ -1 + xpol[,1:4]); b3= coef(fit3)
fit9= lm(y ~ -1 + xpol); b9= coef(fit9)

pred1= b1[1] + xseq*b1[2]
pred3= xseqpol[,1:4] %*% matrix(b3, ncol=1)
pred9= xseqpol %*% matrix(b9, ncol=1)

dataobs= tibble(x,y)  
df= tibble(truemean=sin(2*pi*xseq), x=xseq, pred0=b0, pred1, pred3, pred9)

ggplot(df, aes(x=x)) +
  geom_line(aes(y=truemean)) +
  geom_line(aes(y=pred0), col='gray') +
  geom_line(aes(y=pred1), col='gray') +
  geom_line(aes(y=pred3), col='blue') +
  geom_line(aes(y=pred9), col='red') +
  geom_point(aes(x=x, y=y), data=dataobs) +
  labs(y='True mean')
```


## Over-fitting

The prediction accuracy in our **training data** does not generalize to **new data**

- Adding parameters to a model can only improve MSE in the **training data**

- Instead, assess MSE **out-of-sample**, i.e. on data that were not used to fit the model


Two popular strategies: train/test split and cross-validation


## Train/test split

Use a fraction of the data to fit the model (e.g. 90%), assess MSE on the rest


**Example.** Prices for 100 new condos not in `housing`

```{r}
housingnew= read_csv("../datasets/housingNew.csv")
housingnew= transform(housingnew, lSqFt= log10(SqFt), lUnits= log10(Units))
dim(housingnew)
```

---

1. Fit two models on training data (`housing`)

2. Check accuracy on test data (`housingnew`)

```{r}
fit1= lm(ValuePerSqFt ~ Boro + lSqFt + lUnits , data=housing)
fit2= lm(ValuePerSqFt ~ Boro + lSqFt + lUnits + Boro:lSqFt + Boro:lUnits, data=housing)
pred1= predict(fit1, newdata=housingnew)
pred2= predict(fit2, newdata=housingnew)
ynew= housingnew$ValuePerSqFt
```

MSE for each model

```{r}
mse= c(sqrt(mean((ynew - pred1)^2)), sqrt(mean((ynew - pred2)^2)))
names(mse)= c('model 1','model 2')
round(mse, 3)
```

% explained variance by each model (out-of-sample $R^2$ coefficient)

```{r}
r2= c(cor(ynew, pred1)^2, cor(ynew, pred2)^2)
names(r2)= c('model 1','model 2')
round(r2, 3)
```

## Cross-validation

Issue with training/test split: estimated accuracy depends on what observations were put in each set

**Leave-one-out cross-validation.** For $i=1,\ldots,n$

1. Exclude $y_i$, fit model to other data. 

2. Record $\hat{\beta}^{(i)}$, and $\hat{y}_i= x_i^T \hat{\beta}^{(i)}$

Estimate MSE with $\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}^{(i)})^2$

**K-fold cross-validation.** Same but divide data into $K$ subsets, leave out one subset at a time


## Example

Function `cv.glm` in package `boot` implements cross-validation for GLMs

Re-fit the linear model with `glm`, run `cv.glm` (by default K=n, i.e. leave-one-out)


```{r}
fit1= glm(ValuePerSqFt ~ Boro + lSqFt + lUnits , data=housing)
fit2= glm(ValuePerSqFt ~ Boro + lSqFt + lUnits + Boro:lSqFt + Boro:lUnits, data=housing)
fit1cv= cv.glm(housing, fit1, K=10)
fit2cv= cv.glm(housing, fit2, K=10)
```

We get 2 numbers. The first is the K-fold cross-validated MSE, the second is a bias-adjusted version

```{r}
sqrt(fit1cv$delta)
sqrt(fit2cv$delta)
```


## Binary outcomes

Cross-validation can be used with any measure of accuracy

For binary outcomes, some popular choices

- % of correct classifications

- Area under the ROC curve

- Log-likelihood loss

Interestingly, the last one can be applied to any probability model


## % of miss-classifications

Suppose that we predict $\hat{y}_i=1$ when $\hat{P}(y_i=1) > 0.5$

Function `cost_misclass` at `routines.R` implements this loss
```{r}
cost_misclass
```

`cv.glm` uses MSE by default, but we can specify any loss function. We'll use the % of miss-classifications instead




## Example. Spam filter

Consider 4 models

1. Email characteristics: Additive effect of `to_multiple`, `cc`, `sent_email` 

2. Add pairwise interactions to 1

3. Email charac. + words used: add `image`, `attach`, `dollar`, `winner` to 1

4. Add pairwise interactions to 3

```{r, warning=FALSE}
fit1= glm(spam ~ to_multiple + cc + sent_email, data=email, family=binomial())
fit2= glm(spam ~ (to_multiple + cc + sent_email)^2, data=email, family=binomial())
fit3= glm(spam ~ to_multiple + cc + sent_email + image + attach + dollar + winner, data=email, family=binomial())
fit4= glm(spam ~ (to_multiple + cc + sent_email + image + attach + dollar + winner)^2, data=email, family=binomial())
```

## In-sample accuracy 

Let's compare Models 1 and 4, *in-sample*

```{r}
ypred1= predict(fit1, type='response')
mc1= cost_misclass(email$spam, ypred1, threshold=0.5)
round(mc1,3)
```

```{r}
ypred4= predict(fit4, type='response')
mc3= cost_misclass(email$spam, ypred4, threshold=0.5)
round(mc3,3)
```

Actually, we rarely predict an email to be spam

```{r}
table(email$spam, ypred1>0.5)
```

## Out-of-sample assessment

Call `cv.glm`, setting argument `cost`

```{r, warning=FALSE}
fit1cv= cv.glm(email, fit1, cost=cost_misclass, K=10)
fit2cv= cv.glm(email, fit2, cost=cost_misclass, K=10)
fit3cv= cv.glm(email, fit3, cost=cost_misclass, K=10)
fit4cv= cv.glm(email, fit4, cost=cost_misclass, K=10)

loss= c(fit1cv$delta[1], fit2cv$delta[1], fit3cv$delta[1], fit4cv$delta[1])
names(loss)= c('model 1', 'model 2', 'model 3', 'model 4')
round(loss,4)
```

## Calibrating the threshold

We predicted spam when $\hat{P}(y_i=1) > 0.5$. We could consider a lower threshold than 0.5

```{r}
cost_misclass(email$spam, ypred1, threshold=0.15)
```

```{r}
table(email$spam, ypred1>0.15)
```


## ROC curve

Suppose we take $\hat{y}_i$ when $\hat{\pi}_i= \hat{P}(y_i=1) > t$ for some $t$ 

|              |  $y_i=0$ |  $y_i=1$ |
|--------------|:--------:|:--------:|
|$\hat{y}_i=0$ |    a     |     b    |
|$\hat{y}_i=1$ |    c     |     d    |

- Specificity: among those with $y_i=0$, what proportion gets $\hat{y}_i=0$?

$$
\mbox{Spec}= \frac{a}{a+c}
$$

- Sensitivity: among those with $y_i=1$, what proportion gets $\hat{y}_i=1$?

$$
\mbox{Sens}= \frac{d}{b+d}
$$

## ROC curve

When comparing $\geq 2$ classifiers, it's advisable to consider several thresholds $t$

- Large $t$ reduces sensitivity, but increases specificity

- Small $t$ reduces specificity, but increases sensitivity


Receiving Operator Characteristic (ROC) curve shows spec vs sens for all $t$


```{r}
set.seed(123)
sel= sample(1:nrow(email), size= round(0.9*nrow(email)), replace=FALSE)
emailtrain= email[sel,] #90% for training data
emailtest= email[-sel,] #10% for test data

fit1train= glm(spam ~ to_multiple + cc + sent_email, data=emailtrain, family=binomial())
fit3train= glm(spam ~ to_multiple + cc + sent_email + image + attach + dollar + winner, data=emailtrain, family=binomial())

pi1test= predict(fit1, type='response', newdata=emailtest)
pi3test= predict(fit3, type='response', newdata=emailtest)

roc1= roc(emailtest$spam, pi1test)
roc3= roc(emailtest$spam, pi3test)
```

---

```{r}
plot(roc1, cex.axis=1.4, cex.lab=1.4); lines(roc3, col='red')
legend('bottomright', c('Model 1','Model 3'), lty=1, col=c('black','red'), cex=1.4)
```

ROC curves often compared via Area Under the Curve. Guessing randomly gives AUC=0.5, perfect predictions give AUC=1.

```{r}
c(roc1$auc, roc3$auc)
```




## CV log-likelihood

Idea: reward models that give higher probability/density to the out-of-sample data

Let $Y_i^* \sim \mbox{Bern}(\pi_i)$ for $i=1,\ldots,n^*$ be out-of-sample data, and
$$
\hat{\pi}_i= \frac{1}{1 + e^{-x_i^T \hat{\beta}}}
$$
where $\hat{\beta}$ was estimated from the training data

The Bernoulli likelihood is $P(Y_i^*= a)= \pi_i^{a} (1 - \pi_i)^{1-a}$

- If we observe $Y_i^*=1$, record the reward 
$\log \hat{P}(Y_i^*= 1)= \log \hat{\pi}_i$

- If we observe $Y_i^*=0$, record $\log \hat{P}(Y_i^*=0)= \log (1 - \hat{\pi}_i)$

---

Function `cost_loglik_logistic` at `routines.R` implements the log-likelihood loss

```{r}
cost_loglik_logistic
```








# Best model for explaining

## Model selection consistency

When comparing many models via LRTs 

- Hard to control the false discoveries (type I error)

- Cannot compare non-nested models

Alternatively, one can use methods that select the true model with probability 1 as $n \rightarrow \infty$. We call this property **model selection consistency**

- **Bayesian information criterion (BIC):** popular method that is model selection consistent for GLMs (under mild technical conditions)

- **Akaike information criterion (AIC):** not model selection consistent. Attempts to choose the same model as cross-validation

- Cross-validation is not model selection consistent either

## Information criteria

**Def.** Consider a model $M_k$ with log-likelihood $L_k(\theta_k)$ and parameters $\theta_k$
$$
\begin{aligned}
&\mbox{BIC}_k= - 2 L_k(\hat{\theta}_k) + \log(n) p_k  \\
&\mbox{AIC}_k= - 2 L_k(\hat{\theta}_k) + 2 p_k 
\end{aligned}
$$
where $p_k=\mbox{dim}(\theta_k)$ is the number of parameters in $M_k$ and $n$ the sample size

Model selection strategy: evaluate BIC for all candidate models $M_1,\ldots,M_K$, choose the one with lowest BIC (analogously for AIC)

If this sounds weird, recall that in linear regression $-2 L_k(\hat{\theta}_k)$ is given by the sum of squared residuals
$$
\sum_{i=1}^n (y_i - x_i^T \hat{\beta}_k)^2
$$


## Example. Email data

Functions `BIC` and `AIC` extract the BIC/AIC from a GLM

- Both choose Model 3, as before
- Faster computations (no need to cross-validate)

Warning: AIC fails to approximate cross-validation, unless $n$ is quite large and the model assumptions hold

```{r}
fit1= glm(spam ~ to_multiple + cc + sent_email, data=email, family=binomial())
fit2= glm(spam ~ (to_multiple + cc + sent_email)^2, data=email, family=binomial())
fit3= glm(spam ~ to_multiple + cc + sent_email + image + attach + dollar + winner, data=email, family=binomial())
fit4= glm(spam ~ (to_multiple + cc + sent_email + image + attach + dollar + winner)^2, data=email, family=binomial())

bic= c(BIC(fit1), BIC(fit2), BIC(fit3), BIC(fit4))
aic= c(AIC(fit1), AIC(fit2), AIC(fit3), AIC(fit4))
models= c('model 1','model 2','model 3','model 4')
data.frame(models, bic, aic)
```

## Searching over models

`bestBIC` in package `mombf` searches all possible models (if one can enumerate them)

```{r}
email= transform(email, spam= as.numeric(as.character(spam))) #convert outcome to numeric 0/1 values
fitall= bestBIC(spam ~ to_multiple+cc+sent_email+image+attach+dollar+winner, data=email, family="binomial")
fitall
```

---

Get inference for the top model using `summary`, `coef`, `confint` and `predict` (as usual)

:::panel-tabset

### summary

```{r}
summary(fitall)
```

### coef, confint

```{r}
cbind(coef(fitall), confint(fitall))
```

### predict

```{r}
predict(fitall)[1:5] #predictions for first 5 individuals
```


:::

---

The standard errors for `sent_email` looked weird. Why?

```{r}
table(email$sent_email, email$spam)
```

The estimated spam probability when `sent_email==1` is 0.

The MLE doesn't exist (it occurs at $\hat{\beta}_3=-\infty$), breaking the technical conditions required by asymptotic theory

Bootstrap won't help either. Why?


Possible fix: function `brglm` from package `brglm2` uses a variation of MLE, penalized likelihood, that's more resilient to these issues



# Too many models

---

When $p$ is large we cannot enumerate all $2^p$ models. Classical strategies:

- Stepwise forward: start with no variables, add 1 at a time

- Stepwise backward: start with all variables, drop 1 at a time

These heuristics often work reasonably, but they can get stuck in local modes

We discuss **Markov Chain Monte Carlo (MCMC)** methods for model search


## MCMC model search

MCMC: ample family of algorithms to sample from any probability distribution

- Trick 1: define a probability distribution over models

$$
\pi(M_k) = \frac{e^{-BIC_k/2}}{\sum_{j} e^{-BIC_j/2}}
$$

Small BIC$_k \Rightarrow$ large $\pi(M_k)$. Clearly $\pi(M_k) \in [0,1]$ and $\sum_{k=1}^K \pi(M_k)=1$

The /2 comes from a Bayesian interpretation of the BIC, but one could use other values



- Trick 2: sample from $\pi(M_k)$. The higher $\pi(M_k)$, the higher the probability of sampling this model

Finally, choose model with best $\pi(M_k)$ among the sampled ones

## MCMC

**Goal:** obtain $B$ samples from a distribution $\pi(z_1,\ldots,z_p)$

- MCMC algorithms give **dependent samples** from $\pi$

- Define a Markov Chain with stationary distribution $\pi$


**Gibbs sampler.** A simple type of MCMC

Denote a model by $\gamma=(\gamma_1,\ldots,\gamma_p)$, where 
$$\gamma_j= \begin{cases} 1 \mbox{, if } \beta_j \neq 0 \\ 0 \mbox{, if } \beta_j=0 \end{cases}$$ 

1. Initialize $\gamma_1^{(0)},\ldots,\gamma_p^{(0)}$
2. For $b=1,\ldots,B$, $j=1,\ldots,p$, update $\gamma_j^{(b)}$ from its conditional distrib. given the current values of the other $\gamma_i$'s

## Example

With $p=2$ continuous covariates we have

|   Model        | $\gamma_1$  | $\gamma_2$ |
|----------------|:-----------:|:----------:|
| No covariates  | 0           | 0          |
| Only $x_1$     | 1           | 0          |
| Only $x_2$     | 0           | 1          |
| $x_1$ and $x_2$| 1           | 1          |

Initialize $\gamma_1^{(0)}=0, \gamma_2^{(0)}=0$ (for example)

Set $\gamma_j=1$ with probability

$$
\frac{e^{-\mbox{BIC}_1/2}}{e^{-\mbox{BIC}_0/2} + e^{-\mbox{BIC}_1/2}}
$$
where BIC$_1$ is the BIC for the model with $\gamma_j=1$, BIC$_0$ that for $\gamma_j=0$



## Example

Simulate $n=1000$ observations with $p=50$ covariates

- $x_i \sim N(0, \Sigma)$ with $\sigma_{jj}=1$, $\sigma_{jk}=0.5$

- $\beta=(0,\ldots,0,0.5,0.5,1,1)$, $\sigma^2=1$

```{r}
library(mvtnorm)
n= 1000; p= 50
sigma= diag(p) #identity pxp matrix
sigma[upper.tri(sigma)]= sigma[lower.tri(sigma)]= 0.5
x= rmvnorm(n, sigma=sigma)
beta= matrix(c(rep(0,p-4), c(0.5,0.5,1,1)), ncol=1)
y= x %*% beta + rnorm(n)
```

---

`bestBIC` uses MCMC to explore the models when $p$ is large

```{r}
fit= bestBIC(y, x)
fit
```

The selected model is the correct one, the estimates are very precise

```{r}
coef(fit)
```

---

Model with best BIC

```{r}
fit$topmodel
```

List of visited models and corresponding BIC

```{r}
unique(fit$models) |>
  arrange(bic)
```


# Exercise 1 (to turn in)

Spam data. Consider models `fit1` and `fit3`

```{r, warning=FALSE}
library(openintro) #contains email dataset
fit1= glm(spam ~ to_multiple + cc + sent_email, data=email, family=binomial())
fit3= glm(spam ~ to_multiple + cc + sent_email + image + attach + dollar + winner, data=email, family=binomial())
```

1. Test the null hypothesis $H_0$ that all parameters added by `fit3` are truly 0.

2. Classify as spam whenever $\hat{\pi}_i > 0.15$. Use `cv.glm` to estimate the proportion of miss-classifications out-of-sample by `fit1` and `fit3`. Which model is best?
Hint: re-define `cost_misclass` to use a threshold of 0.15

3. Use `cv.glm` to compare `fit1` and `fit3` according to the log-likelihood loss. Which is best?
Hint: pass our `cost_loglik_logistic` to `cv.glm`


# Exercise 2 (to turn in)

---

Simulate logistic regression data with $n=200$, $p=10$, where truly only covariates 1-5 have an effect.
Covariates are Gaussian, zero mean, pairwise correlations `rho`

```{r}
simdata.logreg= function(n, beta, rho=0.5) {
  require(mvtnorm)
  p= length(beta)
  sigma= diag(p)
  sigma[upper.tri(sigma)]= sigma[lower.tri(sigma)]= rho
  x= rmvnorm(n, sigma=sigma)
  beta= matrix(beta, ncol=1)
  prob= 1/ (1 + exp(-x %*% beta))
  y= rbinom(n=n, size=1, prob=prob)
  return(list(y=y, x=x))
}
```

Simulate data

```{r}
set.seed(123) #set random number generator seed, so you can reproduce my results
beta= c(rep(0,5), rep(1,5))
data= simdata.logreg(n=200, beta=beta, rho=0.5)
```

1. Fit a logistic regression for `data$y` vs. `data$x`. What P-values are <0.05?

2. Use `bestBIC` to get the model with best BIC. What covariates are selected?


## Exercise 2 extension (not to turn in)

The code below repeats the simulation 100 times. 
`selected[i,j]` is `TRUE` if covariate `j` in simulation `i` 

In what % of the simulations was each variable selected?

```{r}
nsim= 100
selected= matrix(NA, nrow=nsim, ncol=length(beta))
colnames(selected)= paste('x',1:ncol(selected),sep='')
for (i in 1:nsim) {
  data= simdata.logreg(n=200, beta=beta, rho=0.5)
  fit= bestBIC(data$y, data$x, family='binomial', verbose=FALSE)
  selected[i,]= fit$varnames %in% fit$topmodel
}
```

Repeat the exercise, now with $n=500$. In what % of the simulations was each variable selected?



