---
title: "Modern Statistical Computing"
subtitle: "5. Basic models"
author: "David Rossell"
institute: "Pompeu Fabra University"
execute:
  echo: true
format:
  revealjs:
    theme: [default, custom.scss]
    scrollable: true
toc: true
toc-depth: 1
number-sections: false
mouse-wheel: true
code-overflow: scroll
code-line-numbers: false
code-copy: true
cache: true
title-slide-style: pandoc
bibliography: references.bib
---

## Reproducing these lecture notes

Load the required R packages

```{r, warning=FALSE}
library(corrplot)
library(coefplot)
library(ggpubr)
library(gapminder)
library(hexbin)
library(modelr)
library(multcomp)
library(tidyverse)
```

We also load auxiliary routines stored at `routines.R` (directory `code`)

```{r}
source('../code/routines.R')
```

You can also source files from RStudio (`Code -> Source File`)


## Models

George Box's famous mantra

> All models are wrong, but some are useful

Its less well-known context

> Now it would be very remarkable if any system existing in the real world could be exactly represented by any simple model. However, cunningly chosen parsimonious models often do provide remarkably useful approximations. For example, the law PV = RT relating pressure P, volume V and temperature T of an "ideal" gas via a constant R is not exactly true for any real gas, but it frequently provides a useful approximation and furthermore its structure is informative since it springs from a physical view of the behavior of gas molecules.

> For such a model there is no need to ask the question "Is the model true?". If "truth" is to be the "whole truth" the answer must be "No". The only question of interest is "Is the model illuminating and useful?".

---

Another quote by Box (he had many!)

> Statisticians, like artists, have the bad habit of falling in love with their models

Meaning that it's important to check whether the model fits the main features of the data

# Linear models

Models help interpret what's going on in a dataset

$$
y_i = \beta_0 + \sum_{j=1}^p \beta_j x_{ij} + \epsilon_i
$$
where $\epsilon_i \sim N(0,\sigma^2)$ indep $i=1,\ldots,n$

- `lm` fits a linear regression by least-squares in R

- `glm` fits generalized linear models (for non-normal outcomes, e.g. binary)

- `gam` fits generalized additive models

$$
y_i= \sum_{j=1}^p f_j(x_{ij}) + \epsilon_i \mbox{, unknown } f_j
$$


## Reminder

$$y= \begin{pmatrix} y_1 \\ \ldots \\ y_n \end{pmatrix};
X=\begin{pmatrix}
1 & x_{11} & \ldots & x_{1p} \\
\ldots \\
1 & x_{n1} & \ldots & x_{np}
\end{pmatrix}=
\begin{pmatrix}
x_1^T \\
\ldots \\
x_n^T \\
\end{pmatrix}
$$

Then (assuming $X^TX$ is invertible)
$$\hat{\beta}= (X^T X)^{-1} X^T y= \arg\min_\beta \sum_{i=1}^n (y_i - x_i^T \beta)^2$$

Further, if the model assumptions hold

$$\hat{\beta} \sim N(\beta, V); V= \sigma^2 (X^T X)^{-1}$$
Which gives confidence intervals and P-values, e.g.
$\hat{\beta}_j \pm 1.96 \sqrt{v_{jj}}$

## Reminder

Sometimes we're interested in linear combinations of parameters

**Result.** Let $Z \sim N(\mu, V)$ be a $p$-dimensional Normal distribution, and
$C$ a $q \times p$ matrix with full rank $q \leq p$. Then
$$
W= C Z \sim N(C \mu, C V C^T)
$$
is a $q$-dimensional normal

**Example.** We seek a 95% CI for $\beta_2 - \beta_1$. Let
$C= \begin{pmatrix}
-1 & 1 & 0 & \ldots & 0 \\
\end{pmatrix}$
Then $$C \begin{pmatrix} \hat{\beta}_1 \\ \hat{\beta}_2 \\ \ldots \\ \hat{\beta}_p \end{pmatrix} = \hat{\beta}_2 - \hat{\beta}_1 \sim N(\beta_2 - \beta_1, \sigma^2 C (X^T X)^{-1} C^T)$$



## Example. Diamonds data

What drives diamond prices?
Exploration suggests that low-quality diamonds are more expensive (worst diamond color is J (yellow-ish), worst clarity is l1)

::: panel-tabset

### Price vs. cut

```{r}
ggplot(diamonds, aes(cut, price)) + geom_boxplot()
```

### Price vs. color

```{r}
ggplot(diamonds, aes(color, price)) + geom_boxplot()
```

### Price vs. clarity

```{r}
ggplot(diamonds, aes(clarity, price)) + geom_boxplot()
```
:::


## Diagnosing the issue

Price is strongly associated with carats (diamond weight)

```{r}
ggplot(diamonds, aes(carat, price)) + 
  geom_hex(bins = 50) +
  geom_smooth(method='lm')
```

---

Carats also associated with cut, color and clarity

::: panel-tabset

### Carats vs. cut

```{r}
ggplot(diamonds, aes(cut, carat)) + geom_boxplot()
```

### Carats vs. color

```{r}
ggplot(diamonds, aes(color, carat)) + geom_boxplot()
```

### Carats vs. clarity

```{r}
ggplot(diamonds, aes(clarity, carat)) + geom_boxplot()
```
:::


---

The most important assumption in a linear model: **linearity!**

```{r}
diamonds2= mutate(diamonds, lprice=log2(price), lcarat=log2(carat))
ggplot(diamonds2, aes(lcarat, lprice)) + 
  geom_hex(bins = 50) +
  geom_smooth(method='lm')
```

log2 facilitates interpretation (+1 in log2 scale $\Rightarrow \times 2$ in original scale)

---

Fit linear model and save residuals

```{r}
lmfit= lm(lprice ~ lcarat, data = diamonds2)
diamonds2= mutate(diamonds2, res= residuals(lmfit))
```

Recall that worst diamond color is J (yellow-ish), worst clarity is l1

::: panel-tabset

### Residuals vs. cut

```{r}
ggplot(diamonds2, aes(cut, res)) + geom_boxplot()
```

### Residuals vs. color

```{r}
ggplot(diamonds2, aes(color, res)) + geom_boxplot()
```

### Residuals vs. clarity

```{r}
ggplot(diamonds2, aes(clarity, res)) + geom_boxplot()
```
:::



## Fitting the full model

```{r}
lmfit2= lm(lprice ~ lcarat + cut + color + clarity, data=diamonds2)
```

Second model has better $R^2$ coefficient. Careful though, comparison not fully reliable due to over-fitting (to be discussed)

```{r}
summary(lmfit)$r.squared
summary(lmfit2)$r.squared
```

Many statistically signif. coefficients in 2nd model

```{r}
summary(lmfit2)
```


---

Let's plot their predictive accuracy

```{r}
diamonds2= mutate(diamonds2, pred1= predict(lmfit), pred2=predict(lmfit2), res2= residuals(lmfit2))
```

:::panel-tabset

### Model 1

```{r}
ggplot(diamonds2, aes(x=pred1, y=lprice)) + geom_point() + geom_abline(color='blue')
```

### Model 2

```{r}
ggplot(diamonds2, aes(x=pred2, y=lprice)) + geom_point() + geom_abline(color='blue')
```

:::


# Extracting inference

---

`summary` and `confint` give $\hat{\beta}_j$, P-values for $H_0:\beta_j=0$ and confidence intervals

```{r}
summary(lmfit2)$coef
confint(lmfit2, level=0.95)
```

---

Easier to define a function that summarizes any fitted model.
I created function `coefSummary` (at `routines.R`) for that purpose

```{r}
coefSummary
```

```{r}
coefSummary(lmfit2)
```


## Organizing code

It's useful to store useful functions separately

- Put `coefSummary` in file `routines.R`. Source it when starting R

```{r, eval=FALSE}
source('../code/routines.R') #change path to directory where routines.R is stored
```


- Create your own R package (to be seen), document and share the code


Note: [tidymodels](https://www.tidymodels.org) and [parsnip](https://parsnip.tidymodels.org) provide a unified interface for many statistical/machine learning models

Note: R package `caret` does as well (see an [introduction](https://topepo.github.io/caret))


## Plotting intervals

`coefplot` helps visualize 95% confidence intervals

```{r}
coefplot(lmfit2, predictors="cut") #show only parameters with name including "cut"
```


# Residual analysis

## Linear model assumptions

- Linearity

- Constant error variance

- Error normality (if $n$ large, only important for predictive intervals)

- Uncorrelated errors

Residuals vs predicted values

```{r}
ggplot(diamonds2, aes(x=pred2, y=res2)) + geom_point()
```


## Assessing linearity

```{r}
ggplot(diamonds2, aes(pred2, res2)) +
  geom_point() +
  geom_smooth() +
  geom_abline(slope=0, intercept=0, col='gray') +
  labs(x='Model prediction', y='Residuals')
```


---

Here non-linearity seems mostly due to (log) carats

```{r}
ggplot(diamonds2, aes(lcarat, lprice)) +
  geom_point() +
  geom_smooth(method='lm', col='blue') +
  geom_smooth(col='red')
```


## Assessing constant variance

```{r}
ggplot(diamonds2, aes(x=pred2, y=res2)) + 
  geom_boxplot(mapping = aes(group = cut_width(pred2, 0.2))) +
  labs(x='Model prediction', y='Residuals')
```



## Residual normality

Note: qq-normal plots require that residuals have 0 mean, variance=1 (function `scale`)

:::panel-tabset

### Histogram (qqplot)

```{r}
library(ggpubr)
ggplot(diamonds2, aes(x=res)) +
  geom_histogram(aes(y= ..density..)) +
  stat_overlay_normal_density(linetype = "dashed") +
  labs(x='Residuals')
```

### Histogram (base R)

```{r}
hist(scale(diamonds2$res), xlab='Residuals', prob=TRUE, main='')
xseq= seq(-4,4,length=200)
lines(xseq, dnorm(xseq))
```
### qq-plot

```{r}
ggplot(diamonds2) +
  geom_qq(aes(sample=scale(res))) +
  geom_abline(slope=1, intercept=0)
```

### qq-plot (base R)

```{r}
qqnorm(scale(diamonds2$res))
abline(0,1)
```


:::

# Exercise

---

FiveThirtyEight's [post](https://fivethirtyeight.com/features/higher-rates-of-hate-crimes-are-tied-to-income-inequality) on hate crimes around the 2016 USA elections

Dataset `hate_crimes` (package `fivethirtyeight`) has this info for each state:

- `avg_hatecrimes_per_100k_fbi`: pre-election number of hate crimes per 100,000 population by the FBI

- education (% adults $\geq$ 25 with high school degree)

- % of people who voted Donald Trump

- Income inequality (Gini index)

Plus variables about racial diversity, economics etc (see `help(hate_crimes)`)

---

Load the data, select and rename the needed variables, and remove Hawaii (outcome not reported)

```{r, warning=FALSE}
library(tidyverse)
library(fivethirtyeight)
source("../code/routines.R")
hc= transmute(hate_crimes, state=state_abbrev, hatecrimes_fbi=avg_hatecrimes_per_100k_fbi, votes_trump=share_vote_trump, gini=gini_index, hs=share_pop_hs)
hc= filter(hc, !is.na(hatecrimes_fbi)) #outcome must be non-missing
```

1. Fit a linear regression for `hatecrimes_fbi` vs. `votes_trump`, `gini`, `hs`. Report the point estimate, 95% intervals and P-values for the 3 covariates. Which of them are significantly associated to hate crimes?

2. Assess the model assumptions: linearity, constant error variance, error normality. To what extent are they violated?

Turn in an html with your solution at Aula Global
Name your file firstname_lastname.html

---

Challenge: all variables are strongly correlated

```{r}
R= select(hc, -state) |>
  cor(use='pairwise.complete.obs')
corrplot.mixed(R)
```








# Interpreting the coefficients

## Factors

To interpret the coefficients of categorical variables, we must understand how they're coded

To avoid problems, let's store them as standard unordered factors

```{r}
unique(diamonds2$cut)
unique(diamonds2$color)
unique(diamonds2$clarity)
```

```{r}
diamonds2= mutate(diamonds2, 
                  cut=factor(cut, ordered=FALSE), 
                  color=factor(color, ordered=FALSE), 
                  clarity=factor(clarity, ordered=FALSE) )
unique(diamonds2$cut)
```

## model.matrix

Check how R codes internally the variables. Let's focus on `cut`

```{r}
x= model.matrix(~ lcarat + cut, data=diamonds2)
x[1:5,]
```

```{r}
unique(levels(diamonds2$cut))
```

For `cut`, "Fair" is the reference category.

---

More precisely, the model is

$$
\mbox{lprice}_i= \beta_0 + \beta_1 \mbox{carat}_i + \beta_2 \mbox{good}_i + \beta_3 \mbox{very good}_i + \beta_4 \mbox{premium}_i + \beta_5 \mbox{ideal}_i + \epsilon_i 
$$
When `cut` is fair and good we get (respectively) 

$$E(\mbox{lprice}_i \mid \mbox{fair}_i=1, \mbox{carat}_i)= \beta_0 + \beta_1 \mbox{lcarat}_i$$

$$
E(\mbox{lprice}_i \mid \mbox{good}_i=1, \mbox{carat}_i)= \beta_0 + \beta_1 \mbox{lcarat}_i + \beta_2
$$

The predicted difference cut - fair diamond is hence $\beta_2$, regardless of carats. To facilitate interpretation, since $\mbox{lprice}_i$ is $\log_2$ price,

$$
\frac{2^{E(\log_2 \mbox{price}_i \mid \mbox{good}_i=1)}}{2^{E(\log_2 \mbox{price}_i \mid \mbox{fair}_i=1)}}= 2^{\beta_2}
$$


##

Estimates and CIs in the log2-scale (recall: `coefSummary` is a function that we defined)

```{r}
lmfit2= lm(lprice ~ lcarat + cut + color + clarity, data=diamonds2)
coefSummary(lmfit2)
```


---

Let's express them in the original scale. Fair -> Good increases price by 1.08 (i.e. by 8%), Fair -> Very good by 1.12 (i.e. by 12%), etc.

```{r}
power2= function(x) 2^x
coefSummary(lmfit2, transform= power2)
```


## Changing the reference category

Set J (worst color) as the reference category for `color` 

```{r}
diamonds2= mutate(diamonds2, color=factor(color, levels=c('J','D','E','F','G','H','I')))
lmfit3= lm(lprice ~ lcarat + cut + color + clarity, data=diamonds2)
coefSummary(lmfit3, transform=power2)
```




## Displaying the predictions

Functions `data_grid` and `add_predictions` in package `modelr` 

- Define grid of predictor values

- Store prediction for each

- Unspecified predictors set to mean  (continuous) or reference category (discrete)

```{r}
mygrid= data_grid(diamonds2, cut, color, clarity, .model=lmfit2)
mygrid
```

---

```{r}
lmpred2= add_predictions(mygrid, model=lmfit2)
lmpred2
```

`add_predictions` requires that `model` is of type `lm` / object that has a `predict` method

---


```{r}
ggplot(lmpred2, aes(x=clarity, y=pred, color=color)) +
  geom_point() +
  facet_wrap(~ cut) +
  labs(y='log2 (price)')
```

# Linear contrasts

## Linear contrasts

We wish to obtain a conf. int. for cut= "Ideal" - "Premium"

- $\beta_5$: mean for "Premium" - mean for "Fair" (reference)

- $\beta_6$: mean for "Ideal" - mean for "Fair" (reference)

- $\beta_6 - \beta_5$: mean for "Ideal" - mean for "Premium"


```{r}
coefSummary(lmfit2)
```

---

**Goal.** Test $H_0: \beta_6 - \beta_5=0$. Get 95% confidence interval

Linear contrasts done by `glht` (general linear hypotheses, R package `multcomp`)

```{r}
C= matrix(0,nrow=1, ncol=length(coef(lmfit2)))
C[1,5]= -1; C[1,6]=1
C[1,1:10] #show first 10 columns
```


```{r}
PvsI= glht(lmfit2, C)
summary(PvsI)
confint(PvsI)
```


## Doing it on our own

Recall that $\hat{\beta} \sim N(\beta, V)$, where $V= \sigma^2 (X^T X)^{-1}$

```{r}
bhat= matrix(coef(lmfit2), ncol=1)
V= vcov(lmfit2)
round(sqrt(diag(V)), 5) #SE's for beta's (compare to summary(lmfit2))
```

Recall that $C \hat{\beta} \sim N(C \beta, C V C^T)$. For us $C \hat{\beta}= \hat{\beta}_6 - \hat{\beta}_5$, $C\beta =\beta_6 - \beta_5$.

```{r}
dhat= C %*% bhat
se= sqrt(C %*% V %*% t(C))
round(se, 5) #matches previous slide
dhat.ci= c(estimate=dhat, ci.low= dhat - 1.96 * se, ci.up= dhat + 1.96 * se)
round(dhat.ci, 5) #matches previous slide
```



# Interactions

---

The effect of a covariate may depend on others

**Example.** Does effect of carats depend on cut?

```{r}
#| code-fold: true
ggplot(diamonds2, aes(lcarat, lprice, color=cut)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE)
```

---

Let's fit a model with interactions. All coefficients are statistically significant

```{r}
lmfit3= lm(lprice ~ lcarat + cut + lcarat:cut, data=diamonds2)
coefSummary(lmfit3)
```

---

We just fitted the model

$$
\begin{aligned}
&\mbox{lprice}_i= \beta_0 + \beta_1 \mbox{lcarat}_i + \beta_2 \mbox{good}_i + \beta_3 \mbox{vgood}_i + \beta_4 \mbox{premium}_i + \beta_5 \mbox{ideal}_i +
\\
&
\beta_6 \mbox{lcarat}_i \mbox{good}_i + \beta_7 \mbox{lcarat}_i \mbox{vgood}_i + \beta_8 \mbox{lcarat}_i \mbox{premium}_i + \beta_9 \mbox{lcarat}_i \mbox{ideal}_i + \epsilon_i
\end{aligned}
$$

For example, when cut="fair"
$$
\mbox{lprice}_i= \beta_0 + \beta_1 \mbox{lcarat}_i
$$
and when cut="good"
$$
\mbox{lprice}_i= \beta_0 + \beta_2 + (\beta_1 + \beta_6) \mbox{lcarat}_i 
$$

How do we interpret $\beta_6$?

---

If $H_0: \beta_6=\beta_7=\beta_8=\beta_9=0$, then no interactions needed

1. Fit models with and without interactions

2. Compare with likelihood-ratio test (=ANOVA for linear models)

```{r}
lmfit4= lm(lprice ~ lcarat + cut, data=diamonds2)
anova(lmfit4, lmfit3)
```


## The full exercise

Consider all possible interactions. All P-values are highly statistically significant

```{r}
lmfull= lm(lprice ~ lcarat + cut + color + clarity + lcarat:cut + lcarat:color + lcarat:clarity, data=diamonds2)
lmdrop1= lm(lprice ~ lcarat + cut + color + clarity + lcarat:color + lcarat:clarity, data=diamonds2)
pvalue1= anova(lmdrop1, lmfull)[['Pr(>F)']][2]

lmdrop2= lm(lprice ~ lcarat + cut + color + clarity + lcarat:cut + lcarat:clarity, data=diamonds2)
pvalue2= anova(lmdrop2, lmfull)[['Pr(>F)']][2]

lmdrop3= lm(lprice ~ lcarat + cut + color + clarity + lcarat:cut + lcarat:color, data=diamonds2)
pvalue3= anova(lmdrop3, lmfull)[['Pr(>F)']][2]
c(pvalue1, pvalue2, pvalue3)
```

However, predictions are nearly identical


```{r}
summary(lmfit2)$r.squared #additive model
summary(lmfull)$r.squared #additive + all pairwise interactions
```

```{r}
diamonds2$predfull= predict(lmfull)
cor(diamonds2$pred2, diamonds2$predfull)
```

---

- Earlier we plotted price vs. carat & cut. `carat:cut` impacted predictions

- Regressing price vs. carat, cut, color & clarity. <interactions don't change predictions much

Why? After adding all covariates, estimated interaction coef. became smaller

::: panel-tabset

### Carat & Cut

```{r, echo=FALSE}
s= coefSummary(lmfit3)
sel= grep("lcarat:",s$Parameter)
knitr::kable(s[sel,])
```

### All covariates

```{r, echo=FALSE}
s= coefSummary(lmfull)
sel= grep("lcarat:",s$Parameter)
knitr::kable(s[sel,])
```


:::

---

`multiplot` visually compares coefficients across models. Interactions lcarat:cut are smaller under the full model

```{r}
multiplot(lmfull, lmfit3, intercept=FALSE)
```


## Specifying interactions in R

We simulate some data for illustration

```{r}
x1= rnorm(100); x2= rnorm(100); x3= rnorm(100)
y= rnorm(100, x1 + x2/2, sd=1)  #y = x1 + x2/2 + e, e ~ N(0,1)
```

Three ways to specify the same regression equation

```{r}
fit1= lm(y ~ x1 + x2 + x3 + x1:x2 + x1:x3 + x2:x3)
fit2= lm(y ~ x1*x2 + x1*x3 + x2*x3) #with * you can drop the additive terms
fit3= lm(y ~ (x1 + x2 + x3)^2) #^2 adds main effects + interactions in one go
fit3
```



# Nested models

---

A particular type of interaction is when data are nested within units (e.g. countries), and we wanna fit a model for each unit

**Example** gapminder data (country life expectancy over time)

```{r}
library(gapminder)
gapminder
```


---

```{r}
ggplot(gapminder, aes(year, lifeExp, group = country)) +
  geom_line() + facet_wrap(~ continent)
```

---

Two ways to fit a model separately for each country: 

- `lm` plus interaction with country. Check which is the reference country, recover intercept/slopes for the rest

- Use `group_by` and `map_df` 

Note: better alternatives to fit nested models (random effects/hierarchical models)

First, group by country and `nest` dataset: each row is a country, and column `$data` is a list of tibbles

```{r}
gm_country= group_by(gapminder, country, continent) |> nest() 
gm_country
```

---

Second, define a function to be applied to each dataset

```{r}
fitmodel= function(df) {
  #Return OLS and R2 for lifeExp ~ year
  fit= lm(lifeExp ~ year, data=df)
  ans= c(coef(fit), summary(fit)$r.squared)
  names(ans)= c('b0','b1','R2')
  return(ans)
}
```

Finally, use `map_df` to apply `fitmodel` to each entry of the list `gm_country$data`

```{r}
coef_gm = map_df(gm_country$data, fitmodel)
coef_gm = cbind(gm_country, coef_gm)
coef_gm
```

---

The $R^2$ coefficient is large for most countries, but pretty small for a few


:::panel-tabset

### $R^2$ histogram

```{r}
ggplot(coef_gm, aes(x=R2)) + geom_histogram()
```

### Countries

```{r}
#dplyr:filter and dplyr::select ensures using functions (filter,select) from package dplyr
dplyr::filter(coef_gm, R2 < 0.5) |> dplyr::select(country, continent, R2)
```

---

:::

---

Unnest the dataset and add predictions

```{r}
coef_gmu= unnest(coef_gm, cols=names(coef_gm))  #unnest & keep all columns
coef_gmu$pred= coef_gmu$b0 + coef_gmu$b1 * coef_gmu$year
coef_gmu
```

---

```{r}
poorfit= filter(coef_gmu, R2 < 0.25) #countries with low R2
ggplot(poorfit) +
  geom_line(aes(year, lifeExp)) +
  geom_line(aes(year, pred), col='red') +
  facet_wrap(~ country)
```

Can you explain any of these patterns?



# Summary of useful functions

## Useful functions

- Fitting the model: `lm`, `predict`, `residuals`

- Extract inference from fitted model: `summary`, `confint`

- Plot coefficients: `coefplot`, `multiplot`

- Plot predictions: `data_grid`, `add_predictions`

- Diagnostics: `geom_smooth`, `geom_boxplot`, `geom_qq`


# Exercise

Select 3 countries, create factor `country` with 3 levels.
Create `years_since_1952`

```{r}
library(tidyverse)
library(gapminder)
source("../code/routines.R")
gm3= gapminder |> 
  filter(country %in% c('France','Italy','Spain')) |>
  transform(country= factor(country), years_since_1952= year - 1952)

fit1= lm(lifeExp ~ years_since_1952 + country, data=gm3)
coefSummary(fit1)
```

- For France: $E(y_i)= \beta_0 + \beta_1 (\mbox{year}_i - 1952)$

- For Italy: $E(y_i)= \beta_0 + \beta_2 + \beta_1 (\mbox{year}_i - 1952)$

How do we interpret $\beta_0$? And $\beta_0 + \beta_2$?

---

`country` has 3 levels. `contrasts` shows how they're coded


```{r}
levels(gm3$country)
contrasts(gm3$country)
```

Recall that $x_{i1}$ is the year. Internally R defines

::: columns
::: {.column width="50%"}

$$ 
x_{i2}= \begin{cases} 1 \mbox{, if country}_i=\mbox{Italy} \\ 0 \mbox{, else} \end{cases} 
$$
:::

::: {.column width="50%"}

$$ 
x_{i3}= \begin{cases} 1 \mbox{, if country}_i=\mbox{Spain} \\ 0 \mbox{, else} \end{cases} 
$$

:::

:::

Alternative to 0/1 coding: **sum-to-zero constraint**. In our example

::: columns
::: {.column width="50%"}

$$ 
x_{i2}= \begin{cases} 1 \mbox{, if country}_i=\mbox{France} \\ -1 \mbox{, if country}_i=\mbox{Spain} \\ 0 \mbox{, else} \end{cases} 
$$
:::

::: {.column width="50%"}

$$ 
x_{i3}= \begin{cases} 1 \mbox{, if country}_i=\mbox{Italy} \\ -1 \mbox{, if country}_i=\mbox{Spain}  \\ 0 \mbox{, else} \end{cases} 
$$
:::

:::


---

sum-to-zero coding can be done with `contrasts` and `contr.sum`

```{r}
contr.sum(levels(gm3$country)) #check what the sum-to-zero codes look like
```

```{r}
gm3$country0= gm3$country #create new variable
c0= contr.sum(levels(gm3$country0)) #tell contr.sum the factor levels
colnames(c0)= c('France','Italy')   #set column names to facilitate interpreting coef.
contrasts(gm3$country0) = c0
```

Let's fit the model with the new coding

```{r}
fit2= lm(lifeExp ~ years_since_1952 + country0, data=gm3)
coefSummary(fit2)
```

Results for $\beta_1$ (years) as before. Different for $(\hat{\beta}_0, \hat{\beta}_2, \hat{\beta}_3)$


## Interpretation

$$
E(y_i)= \beta_0 + \beta_1 (x_{i1} - 1952) + \beta_2 x_{i2} + \beta_3 x_{i3}
$$



::: columns
::: {.column width="50%"}

$$ 
x_{i2}= \begin{cases} 1 \mbox{, if country}_i=\mbox{France} \\ -1 \mbox{, if country}_i=\mbox{Spain} \\ 0 \mbox{, else} \end{cases} 
$$
:::

::: {.column width="50%"}

$$ 
x_{i3}= \begin{cases} 1 \mbox{, if country}_i=\mbox{Italy} \\ -1 \mbox{, if country}_i=\mbox{Spain}  \\ 0 \mbox{, else} \end{cases} 
$$
:::

:::


- For France: $E(y_i)= \beta_0 + \beta_2 + \beta_1 (x_{i1} - 1952)$

- For Italy: $E(y_i)= \beta_0 + \beta_3 + \beta_1 (x_{i1} - 1952)$

- For Spain: $E(y_i)= \beta_0 - \beta_2 - \beta_3 + \beta_1 (x_{i1} - 1952)$

- Average across countries: $E(y_i)= \beta_0 + \beta_1 (x_{i1} - 1952)$

$\beta_0$ is the mean life exp. at 1952 across countries

$\beta_2$ is France's deviation from that mean. $\beta_3$ is Italy's. $-\beta_2 - \beta_3$ is Spain's

## Exercise

Fit a model with interactions with the 0/1 coding and with the sum-to-zero coding. Interpret the estimated parameter values from each model

```{r}
fit3= lm(lifeExp ~ years_since_1952 + country + years_since_1952:country, data=gm3)
fit4= lm(lifeExp ~ years_since_1952 + country0 + years_since_1952:country0, data=gm3)
```

