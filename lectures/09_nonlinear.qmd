---
title: "Modern Statistical Computing"
subtitle: "09. Non-linear models"
author: "David Rossell"
institute: "Pompeu Fabra University"
execute:
  echo: true
format:
  revealjs:
    theme: [default, custom.scss]
    scrollable: true
toc: true
toc-depth: 1
number-sections: false
mouse-wheel: true
code-overflow: scroll
code-line-numbers: false
code-copy: true
cache: true
title-slide-style: pandoc
bibliography: references.bib
---

## Reproducing these lecture notes

Load required R packages

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(keras)
library(mlbench)
library(mgcv)
library(tensorflow)
library(reshape2)
library(caret)
library(randomForest)
library(randomForestExplainer)
library(rpart)
library(rpart.plot)
library(plotROC)
#install_tensorflow()  #run this the first time you use tensorflow
source("../code/routines.R")
```

We use `dplyr::select` to avoid tidyverse's `select` (`dplyr` package) to be confused with `select` defined in other packages

## Non-linear models

In some applications it can be critical to capture non-linear effects. We already saw two basic strategies

- Add quadratic / polynomial terms $x_{ij}^2, x_{ij}^3, \ldots$

- Discretize $x_{ij}$ into several groups

More refined strategies (e.g. used by `ggplot`)

- Additive regression

- Deep learning

- Regression trees / random forests

- ...




# Generalized additive models

---

Additive linear regression. $y_i \sim N(\mu_i, \sigma^2)$,

$$ y_i = \sum_{j=1}^p f_j(x_{ij}) + \epsilon_i= \mu_i + \epsilon_i $$


Additive logistic regression. $y_i \sim \mbox{Bern}(\mu_i)$


$$ \log \left( \frac{\mu_i}{1 - \mu_i} \right) = \sum_{j=1}^p f_j(x_{ij}) $$

Additive Poisson regression. $y_i \sim \mbox{Poisson}(\mu_i)$

$$ \log \mu_i = \sum_{j=1}^p f_j(x_{ij}) $$

## Example

GAMs can be fitted with function `gam` (package `mgcv`)

```{r}
n= 50; x= seq(0,2*pi,length=n); y= sin(x) + rnorm(n, sd=.2)
fit= gam(y ~ s(x))
```

:::panel-tabset

### Model summary

```{r}
summary(fit)
```

### Plot

```{r}
plot(x, y); lines(x, sin(x), col='blue'); lines(x, predict(fit))
legend('topright',c("Truth","Estimated"),lty=1,col=c("blue","black"))
```

:::




## Additive models via basis functions

Idea: transform $x_{ij} \in \mathbb{R}$ into vector $w_j(x_{ij}) \in \mathbb{R}^L$. Let
 
$$
\sum_{j=1}^p f_j(x_{ij})= \sum_{j=1}^p w_j(x_{ij})^T \beta_j
$$

**Examples.**

- Quadratic terms. $w_j(x_{ij})^T= (x_{ij}, x_{ij}^2)$

- Discretize into $K$ groups. $w_j(x_{ij})^T= (0,\ldots,0,1,0,\ldots,0)$ (1 indicates $x_{ij}$'s group)

More advanced examples: splines, Fourier basis etc.


## Splines


**Def.** Let $x \in \mathbb{R}$. $f(x)$ is a spline of degree $d$ and knots $\nu_1 < \ldots < \nu_K$ iff

- $f(x)$ is a degree $d$ polynomial in each interval $(\nu_k,\nu_{k+1})$

- $f(x)$ has $d-1$ continuous derivatives at $\nu_1,\ldots,\nu_K$

It's easy to obtain splines. Consider
$$f(x)= \sum_{l=1}^L w_l(x) \beta_l$$ 
where $w_l$'s are degree $d$ polynomials and $\beta_l$'s satisfy certain restrictions, ensuring that $f(x)$ has $d-1$ continuous derivatives


## Example. Degree 1 B-splines

$$f(x)= w_1(x) + 1.1 w_2(x) +1.5 w_3(x) + 1.6 w_4(x)$$

B-splines: minimal support & guarantee $d-1$ continuous derivatives

::: {layout-ncol=2}

![](figs/splinelinear_basis.jpeg)

![](figs/splinelinear_fx.jpeg)

:::


## Model fitting

Since $\sum_{j=1}^p f_j(x_{ij})= \sum_{j=1}^p w_j(x_{ij})^T \beta_j$, we can write
$$
y= W \beta + \epsilon
$$
where $y, \epsilon \in \mathbb{R}^n$, $W$ contains all the $w_j(x_{ij})$'s and $\beta^T= (\beta_1^T,\ldots,\beta_p^T) \in \mathbb{R}^{pL}$ 

A standard linear regression model!

- In principle, we could use least-squares

- Many parameters ($pL$), use generalized cross-validation to avoid over-fitting

GCV is a computationally faster alternative to cross-validation


## Example. Diamonds data

```{r}
#| code-fold: true

diamonds2= mutate(diamonds, lprice=log2(price), lcarat=log2(carat))
ggplot(diamonds2, aes(lcarat, lprice)) + 
  geom_hex(bins = 50) +
  geom_smooth(method='lm') +
  geom_smooth(color='black') +
  labs(x='log2 carats', y='log2 diamond price')
```

---

```{r}
fit1= gam(lprice ~ lcarat + cut + color + clarity, data= diamonds2)     #linear regression
fit2= gam(lprice ~ s(lcarat) + cut + color + clarity, data= diamonds2)  #GAM
```

:::panel-tabset

### fit1

```{r}
summary(fit1)
```

### fit2

```{r}
summary(fit2)
```

:::





# Deep learning

##

Neural networks can capture very general non-linear patterns

```{r, echo=FALSE}
library(igraph)

#Define node locations
layout= cbind(c(0,0,rep(1,10),rep(2,10),3), c(-2,2,-4:5,-4:5,0))

#Define edges between multiple layers
edge1= cbind(rep(1:2,each=10),rep(3:12,2))
edge2= as.matrix(expand.grid(3:12,13:22))
edge2= edge2[edge2[,1] < edge2[,2],]
edge3= cbind(rep(13:22), 23)
edge= rbind(edge1, edge2, edge3)

#Plot network
g= graph.data.frame(edge, directed=TRUE)
par(mar=rep(0,4), oma=rep(0,4))
plot(g, layout=layout, vertex.label=NA, xlim=c(-1,1), ylim=c(-1.1,1.1), asp=0)
text(-1,1.2,"Covariates", cex=1.25)
text(-0.33,1.2,"Layer 1", cex=1.25)
text(0.33,1.2,"Layer 2", cex=1.25)
text(1,1.2,"Output", cex=1.25)
```


## In a nutshell

Value of node $k$ in layer $j$: linear combination of incoming nodes from layer $j-1$, apply an *activation function*

Let $x^{(0)}$ be the input covariates. For layer $k$, nodes $j=1,\ldots,N_k$ are
$$ x_j^{(k)}= g\left( \beta_{kj0} + \beta_{kj}^T x^{(k-1)} \right)$$

Popular activation functions $g()$: ReLu, softmax, sigmoid

Last layer $K+1$ has 1 node: the outcome
$\hat{y}= \beta_{K+1, 0} + w^T_{K+1} x^{(K)}$

Finally, specify loss function: least-squares, logistic regression log-likelihood

**Estimation:** stochastic gradient descent [video](https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=3&ab_channel=3Blue1Brown) (first 3 minutes)


## Practical issues

- Many parameters ($K$ layers with $N$ nodes has order $p N + K N^2$)

- Many tuning parameters: number of nodes, layers. Train/test data split

- Loss function has many local modes. We'll trust stochastic gradient descent (run the model several times for safety)


Further resources

- [Getting started with deep learning in R](https://posit.co/blog/getting-started-with-deep-learning-in-r) guide

- [Deep learning book](https://srdas.github.io/DLBook) (Chapter 10)



## Breast cancer data

As illustration we use a binary outcome example, see [here](https://tensorflow.rstudio.com/tutorials/keras/regression) for a continuous outcome example

- Outcome: malignant breast cancer

- 9 covariates measuring cell characteristics

- Sample size $n=699$


## Pre-processing

Convert to numeric, standardize covariates to mean 0 & variance 1 (important!), train/test split

```{r}
data("BreastCancer")  #package mlbench
BreastCancer = BreastCancer[which(complete.cases(BreastCancer)==TRUE),] #exclude cases with missing values
BreastCancer = mutate(BreastCancer, Class= ifelse(Class=='malignant',1,0)) |>
  mutate_if(is.factor, as.numeric) |> #convert factors to numeric
  dplyr::select(-Id)  #drop Id column

covars= names(dplyr::select(BreastCancer, -Class))
BreastCancer= mutate_at(BreastCancer, covars, scale) #standardize to mean 0, variance 1

n= nrow(BreastCancer)
sel= sample(1:n, size=round(0.8*n), replace=FALSE)  #80% of data in training set, 20% in test set
train= BreastCancer[sel,]
test= BreastCancer[-sel,]

Xtrain= data.matrix(dplyr::select(train, -Class))
ytrain= data.matrix(dplyr::select(train, Class))

Xtest= data.matrix(dplyr::select(test, -Class))
ytest= data.matrix(dplyr::select(test, Class))

```


## Setting up Tensorflow

Define the model: 2 hidden layers with 100 units each

```{r}
nunits= 100

model= keras_model_sequential() |>
  layer_dense(units= nunits, activation='relu') |>
  layer_dense(units= nunits, activation='relu') |>
  layer_dense(units=1, activation='sigmoid')
```

Compile the model

```{r}
model |> compile(
  loss = 'binary_crossentropy',    #logistic regression loss
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
```

Fit the model

```{r}
model |> fit(Xtrain, ytrain, epochs = 100, verbose = 0, validation_split = 0.1)
```

## Result accuracy

```{r}
ytestpred= predict(model, Xtest)

boxplot(ytestpred ~ ytest, xlab='Malignant tumor', ylab='Deep learning prediction')
```

```{r}
table(ytestpred > 0.5, ytest)
```


---

```{r}
pred= data.frame(predict(model, Xtrain)[,1], Xtrain) |> as_tibble()
names(pred)= c("ypred",colnames(Xtrain))
```


```{r}
ggplot(pred, aes(Cl.thickness, ypred)) + geom_point() + geom_smooth()
```

## All covariates

Trick: re-format data and facet by covariate

```{r}
pred
```


```{r}
predlong= melt(pred, id.vars="ypred")  #from package reshape2
predlong
```

---

```{r, warning=FALSE}
ggplot(predlong, aes(value, ypred)) + geom_point() + geom_smooth() + facet_wrap(~ variable)
```


## Compare with a GAM

Let's fit a logistic GAM. Variable `Mitosis` has most data in 1 category, set a linear effect to avoid `gam` returning an error


```{r}
traindata= data.frame(y=ytrain, Xtrain)
testdata= data.frame(y=ytest, Xtest)

gamfit= gam(Class ~ s(Cl.thickness) + s(Cell.size) + s(Cell.shape) + s(Marg.adhesion) + s(Epith.c.size) + s(Bare.nuclei) + s(Bl.cromatin) + s(Normal.nucleoli) + Mitoses, data=traindata, family=binomial())

gampred= predict(gamfit, newdata=testdata, type='response')
```

---

:::panel-tabset


### Test set

```{r}
boxplot(gampred ~ testdata$Class, xlab='True class', ylab='GAM prediction', cex.lab=1.3)
```

### GAM vs DL (train)

```{r}
col= ifelse(ytrain[,1]==1, 'red','black')
plot(expit(predict(gamfit)), pred$ypred, xlab='GAM', ylab='Deep learning', col=col, main='Training set')
legend('topleft', c('Malign','Benign'), pch=1, col=c('red','black'))
```


### GAM vs DL (test)

```{r}
col= ifelse(ytest[,1]==1, 'red','black')
plot(gampred, ytestpred, xlab='GAM', ylab='Deep learning', col=col, main='Test set')
legend('topleft', c('Malign','Benign'), pch=1, col=c('red','black'))
```

:::


---

Both models predict very similarly in the test sample. Which one would you use?

```{r}
table(ytestpred > 0.5, ytest[,1])
```


```{r}
table(gampred > 0.5, ytest[,1])
```




## Exercise 1. Diamonds data

Produce a residuals $\hat{y} - y$ vs. predictions $\hat{y}$ plot for the linear and GAM models


1. Does the GAM improve the linearity assumption relative to the linear model?
In particular, discuss any systematic biases in over- or under-predicting some of the diamonds

2. Does the GAM improve the error normality assumption?

Getting started

```{r, eval=FALSE}
library(tidyverse)
library(mgcv)
diamonds2= mutate(diamonds, lprice=log2(price), lcarat=log2(carat))
```



## Exercise 2. Breast cancer

Fit a standard logistic regression model to the breast cancer data, using the same training sample defined above. 

1. Are its predictions similar to those of the GAM and neural network?

2. Is the logistic regression more, less, or similarly accurate?


Turn in a single html with your solution for both exercises at Aula Global
Name your file firstname_lastname.html



# Trees


## Classification and regression trees

Predict $y_i$ as a piecewise-constant function of $x_i$, allowing for interactions

```{r, echo=FALSE}
library(igraph)

#Define node locations
layout= cbind(c(0,-1,1,-2,0), c(0,-1,-1,-2,-2))

#Define edges between multiple layers
edge= rbind(c(1,2), c(1,3), c(2,4), c(2,5))

#Plot network
g= graph.data.frame(edge, directed=TRUE)
par(mar=rep(0,4), oma=rep(0,4))
plot(g, layout=layout, vertex.label=NA, xlim=c(-1,1), ylim=c(-1.1,1.1), asp=0)
text(-0.3, 0.3, expression(x[1] <= 5), cex=1.25)
text(1, 0.3, expression(x[1] > 5), cex=1.25)
text(-1, -0.7, expression(x[2] == 0), cex=1.25)
text(0.35, -0.7, expression(x[2] == 1), cex=1.25)
text(-1, -1, expression(beta[1]), cex=1.25)
text(0.33, -1, expression(beta[2]), cex=1.25)
text(1, 0, expression(beta[3]), cex=1.25)
```

- Continuous $y_i$: $\hat{\beta}_j$'s are sample means

- Binary $y_i$: $\hat{\beta}_j$'s are the proportion of 1's


---

CART typically fit in a greedy fashion. At each split:

- Take variable $j$ and threshold $\tau_j$ reducing MSE (or suitable loss function) the most

- Binary split $x_{ij} > \tau_j$

- Note: variable $j$ can be used again in future splits

Continue until all nodes have few indiv. / meets a purity criterion, then prune

Sufficiently deep trees: small bias, but large variance

**Idea:** build several trees and average to reduce variance

- Bagging (bootstrap aggregation)

- Random forests

- Boosting


## Bagging and variance reduction

 Bagging: for each bootstrap sample $b=1,\ldots,B$

- Train tree, obtain $\hat{y}_i^{(b)}$
- Report $\hat{y}_i= \frac{1}{B} \sum_{b=1}^B y_i^{(b)}$

**Result.** If each tree is identically distrib with $V(\hat{y}_i^{(b)})= \sigma^2$ and $\mbox{Cor}(\hat{y}_i^{(b)}, \hat{y}_i^{(b')})=\rho$
$$
V(\hat{y}_i)= \rho \sigma^2 + \frac{(1 - \rho)}{B} \sigma^2
$$

- As $B \rightarrow \infty$, $V(\hat{y}_i)$ does not vanish

- Random forests reduce $\rho$ using different variables in each tree


## Random forests

For each boostrap sample $b=1,\ldots,B$

1. Select $m$ variables at random (`mtry` parameter)
2. Train tree, obtain $\hat{y}_i^{(b)}$
3. Report $\hat{y}_i= \frac{1}{B} \sum_{b=1}^B y_i^{(b)}$
4. Store error for all samples, including out-of-bag (OOB) samples

Typically $m$ is small (so $\rho$ is also small) and set via cross-val


## Example: Titanic

Outcome: did passenger survive? Data from $n=891$ passengers

- Gender
- Ticket class (1st, 2nd, 3rd)
- Number of siblings/spouses aboard the Titanic
- Number of parents/children aboard
- Passenger fare	
- Embarkation port (Cherbourg, Queenstown, Southampton)

500 trees, `mtry`=1,...,8 variables for each tree (5-fold cross-val)

OOB estimated error rate: 19% 

---

Code outcome as factor, gender as numeric, exclude age (many NA's, no marginal association with the outcome)

```{r, message=FALSE}
data= read.table('../datasets/titanic.csv',sep=',',header=TRUE)
data= transform(data, Survived= factor(data$Survived), female= ifelse(Sex=='female', 1, 0)) |>
  dplyr::select(-Age)  #use dplyr::select over select, other packages also define a select function
```

Package `caret` offers unified interface for many methods

- `method="rf` for random forests
- `trControl` to tune parameters via 5-fold cross-validation


```{r}
tuneGrid= data.frame(mtry=1:8)
trControl=trainControl(method='cv',number=5)
model.rf= caret::train(Survived ~ Pclass + female + SibSp + Embarked + Parch + Fare, data=data, method='rf', trControl=trControl, tuneGrid=tuneGrid)
model.rf
```

## Accuracy vs number of variables (`mtry`)

```{r}
ggplot(model.rf)  #plot(model.rf) also works
```

## Interpreting the predictions

`rpart` fits a single tree. `rpart.plot` gives, 
for each node: (1) predicted class; (2) proportions of 1's; (3) % observations in the node

:::panel-tabset

### Tree for observed outcome

```{r}
fit.tree = rpart(Survived ~ Pclass + Sex + SibSp + Embarked + Parch + Fare, data=data, method = "class")
rpart.plot(fit.tree)
```

### Tree for RF prediction

`predict` extracts predictions. `type="raw"` for predicted 0/1, `type="prob"` for class probabilities (enough to keep the 2nd one)

```{r}
data= transform(data, 
                ypred.rf= predict(model.rf, newdata=data, type='raw'), 
                prob.rf= predict(model.rf, newdata=data, type='prob')[,2])
fit.tree = rpart(ypred.rf ~ Pclass + Sex + SibSp + Embarked + Parch + Fare, data=data, method = "class")
rpart.plot(fit.tree)
```

:::


---

For further summaries, we use `randomForest` 

```{r}
rf= randomForest(Survived ~ Pclass + female + SibSp + Embarked + Parch + Fare, data=data, ntree=500, mtry=3)
rf
```
## Accuracy vs. number of trees

```{r}
plot(rf, cex.lab=1.3, cex.axis=1.3)
legend('topright', c('Overall','Class 0','Class 1'), col=1:3, lty=1:3, cex=1.3)
```

## Measuring variable importance

Mean decrease in accuracy when not using each variable. Accuracy measured with MSE (continuous) or Gini index (binary, based on $-\log(p (1-p))$) 

:::panel-tabset

### Importance

```{r}
rf$importance #measures mean decrease in accuracy when not using each variable
```
### Plot

```{r}
varImpPlot(rf)
```

:::

---

`randomForestExplainer` package: further importance measures for each variable. How often it's the root of a tree, average minimal depth

```{r}
importance_frame= measure_importance(rf)
importance_frame
```


# caret

---

`train` can fit many predictive models. Type `names(getModelInfo())` for a list

- Logistic regression: `method=glm` with `family="binomial"` 

- GAM logistic regression: `method=gam` with `family="binomial"`  

- Neural networks: `method=nnet` for 1 hidden layer, `method=mlpML` for >1 layers


Resources

- [Brief introduction](https://cran.r-project.org/web/packages/caret/vignettes/caret.html)

- [Main help pages](https://topepo.github.io/caret)


## Example: Titanic data


:::panel-tabset

### RF

```{r}
model.rf  #Random forest fitted previously
```

### Log. reg.

```{r}
model.logreg= caret::train(Survived ~ Pclass + female + SibSp + Embarked + Parch + Fare, data=data, family='binomial', method='glm', trControl=trControl)
model.logreg
```

### GAM

```{r}
#library(RSNNS)
model.gam= caret::train(Survived ~ Pclass + female + SibSp + Embarked + Parch + Fare, data=data, family='binomial', method='gam', trControl=trControl)
model.gam
```

### Neural net

```{r}
model.nnet= caret::train(Survived ~ Pclass + female + SibSp + Embarked + Parch + Fare, data=data, family='binomial', method='nnet', trace=FALSE, trControl=trControl)
model.nnet
```



:::


## ROC

To store measures of performance in `trainControl` we set `summaryFunction`, a function taking outcome and predictions and returning measures of performance

For example, `twoClassSummary` gives area under the ROC curve, sensitivity and specificity. The ROC needs predicted probabilities, we store them with `classProbs=TRUE`. We must also specify `metric="ROC"` 

```{r}
trControl=trainControl(method='cv',number=5, classProbs=TRUE, summaryFunction=twoClassSummary)
data= transform(data, SurvivedTxt= ifelse(Survived==1,'Yes','No'))
```

:::panel-tabset

### Logistic regr.

```{r}
model.logreg= caret::train(SurvivedTxt ~ Pclass + female + SibSp + Embarked + Parch + Fare, data=data, family='binomial', method='glm',  trControl=trControl, metric='ROC')
model.logreg
```

### RF

```{r}
model.rf= caret::train(SurvivedTxt ~ Pclass + female + SibSp + Embarked + Parch + Fare, data=data, method='rf', trControl=trControl, metric='ROC', tuneGrid=tuneGrid)
model.rf
```

:::


## Train-test split

`createDataPartition` splits into train/test. We set 75%/25%

```{r}
inTrain= createDataPartition(y=data$Survived, p=0.75, list=FALSE)
training= data[inTrain,]
testing= data[-inTrain,]
```

```{r}
model.logreg.train= caret::train(SurvivedTxt ~ Pclass + female + SibSp + Embarked + Parch + Fare, data=training, family='binomial', method='glm',  trControl=trControl, metric='ROC')
model.rf.train= caret::train(SurvivedTxt ~ Pclass + female + SibSp + Embarked + Parch + Fare, data=training, method='rf', trControl=trControl, metric='ROC', tuneGrid=tuneGrid)
```

Store predictions in `testing` (out of sample)

```{r}
testing= transform(testing,
        prob.rf= predict(model.rf.train, newdata=testing, type='prob')[,2],
        prob.logreg= predict(model.logreg.train, newdata=testing, type='prob')[,2])
```

Store predictions for all data (in-sample!)

```{r}
data= transform(data,
        prob.rf= predict(model.rf, newdata=data, type='prob')[,2],
        prob.logreg= predict(model.logreg, newdata=data, type='prob')[,2])
```



## ROC curves

`geom_roc` from package `plotROC` gives nice-looking plots.

Careful: in-sample ROC for random forests is overly optimistic 


:::panel-tabset

### Test data

```{r}
ggplot(testing, aes(d= ifelse(Survived=='1',1,0))) +  #d is the observed outcome
  geom_roc(aes(m= prob.logreg)) +                  #m is the predicted probability
  geom_roc(aes(m= prob.rf), color='blue')
```

### All data

```{r}
ggplot(data, aes(d= ifelse(Survived=='1',1,0))) +  #d is the observed outcome
  geom_roc(aes(m= prob.logreg)) +                  #m is the predicted probability
  geom_roc(aes(m= prob.rf), color='blue')
```

:::


## Confusion matrix

:::panel-tabset

### Logistic regr.

```{r}
testing= transform(testing, ypred.logreg= factor(ifelse(prob.logreg > 0.5, 1, 0)))
confusionMatrix(testing$ypred.logreg, testing$Survived)
```


### RF

```{r}
confusionMatrix(testing$ypred.rf, testing$Survived)
```

:::


## Exercise. Breast cancer data


Use `caret` to do the following analyses:

1. Use random forests to predict breast cancer

2. Compare out-of-sample accuracy vs. logistic regr (80% training, 20% test)

3. Plot out-of-sample ROC curve for both methods


Getting started

```{r}
data("BreastCancer")  #package mlbench
BreastCancer = BreastCancer[which(complete.cases(BreastCancer)==TRUE),] #exclude cases with missing values

inTrain= createDataPartition(y=BreastCancer$Class, p=0.8, list=FALSE)
training= BreastCancer[inTrain,]
testing= BreastCancer[-inTrain,]
```

