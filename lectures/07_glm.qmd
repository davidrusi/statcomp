---
title: "Modern Statistical Computing"
subtitle: "7. Generalized Linear Models"
author: "David Rossell"
institute: "Pompeu Fabra University"
execute:
  echo: true
format:
  revealjs:
    theme: [default, custom.scss]
    scrollable: true
toc: true
toc-depth: 1
number-sections: false
mouse-wheel: true
code-overflow: scroll
code-line-numbers: false
code-copy: true
cache: true
title-slide-style: pandoc
bibliography: references.bib
---

## Reproducing these lecture notes

Load required R packages

```{r, warning=FALSE}
library(tidyverse)
library(boot)
library(coefplot)
library(modelr)
library(multcomp)
library(openintro)
```

We also source routines.R, which has auxiliary functions

```{r}
source('../code/routines.R')
```


##

Many tasks in data analysis and decision-making require optimizing a function

We focus on cases where the function is a criterion to fit the data

**Example.** Least-squares criterion
$$
\min_\beta \sum_{i=1}^n (y_i - x_i^T \beta)^2
$$

## Maximum likelihood estimation

Given a model for the observed data $y=(y_1,\ldots,y_n)$, set parameter value $\theta$ making $y$ as "probable" as possible

**Def.** The *likelihood function* is the joint density/mass function $p(y \mid \theta)$, seen as a function of $\theta$

**Def.** The MLE is $\hat{\theta}= \arg\max_\theta p(y \mid \theta)$

Technical note: we assume that the maximum exists

## Least-squares as MLE

Let $y_i \sim N(x_i^T \beta, \sigma^2)$ indep. $i=1,\ldots,n$, then
$$
p(y \mid \beta,\sigma^2)= \prod_{i=1}^n p(y_i \mid \beta,\sigma^2)=
\frac{1}{(2\pi\sigma^2)^{n/2}} \exp \left\{ -\frac{1}{2 \sigma^2} \sum_{i=1}^n (y_i - x_i^T \beta)^2  \right\}
$$

Then $(\hat{\beta},\hat{\sigma}^2)$ obtained by maximizing
$$
\log p(y \mid \beta,\sigma^2)=
- \frac{n}{2} \log(2\pi\sigma^2) -\frac{1}{2 \sigma^2} \sum_{i=1}^n (y_i - x_i^T \beta)^2
$$

Equivalently, $\hat{\beta}$ minimizes $\sum_{i=1}^n (y_i - x_i^T \beta)^2$


## Properties

Assume that data truly generated from $p(y \mid \theta^*)$, where $\log p(y \mid \theta)$ satisfies certain regularity conditions (e.g. twice continuously differentiable) and $\theta^*$ is in the interior of the parameter space

As $n \rightarrow \infty$,
$$
\hat{\theta} \stackrel{D}{\longrightarrow} N(\theta^*, H(\theta^*)^{-1}) 
$$
$H(\theta^*)$ is the hessian of $\log p(y \mid \theta)$ (Fisher information matrix) evaluated at $\theta^*$

Hence $\hat{\theta}$ attains smallest variance among all asymptotically unbiased estimators (Cramer-Rao lower bound)


# Logistic regression

## Logistic regression

Let $Y_i \sim \mbox{Bern}(\pi_i)$, where
$$
\log \left( \frac{\pi_i}{1 - \pi_i} \right)= x_i^T \beta
\Longleftrightarrow
\pi_i = \frac{1}{1 + e^{-x_i^T \beta}}
$$
To estimate $\beta$ we consider MLE. Recall: if $Y_i \sim \mbox{Bern}(\pi_i)$,
$$
P(Y_i=y_i)= \pi_i^{y_i} (1-\pi_i)^{1-y_i} = \begin{cases} \pi_i \mbox{ if } y_i=1 \\ 1-\pi_i \mbox{ if } y_i=0 \end{cases}
$$
where $y_i$ is the observed value of $Y_i$

## Log. reg. likelihood

$$
\begin{aligned}
&\log p(y \mid \beta)= \sum_{i=1}^n \log \left( \pi_i^{y_i} (1-\pi_i)^{1-y_i} \right) \\
&=\sum_{i=1}^n y_i \log \left( \frac{\pi_i}{1-\pi_i} \right) + \log(1-\pi_i) 
=\sum_{i=1}^n y_i x_i^T \beta - \log(1 + e^{x_i^T \beta})
\end{aligned}
$$

No closed-form solution. However, 

- $\log p(y \mid \beta)$ has a negative-semidefinite hessian $H(\beta)$ (it's concave)

- Strictly concave, if $X$ has full column rank ($X^TX$ invertible)

- Many efficient numerical optimization algorithms


## Example. Spam filter

The dataset `email` (package `openintro`) has $n=3921$ emails

- $y_i$: email is spam yes/no

- $x_i$: $p=20$ covariates (number of characters, whether email had "Re:" in the subject etc.)

```{r}
email
```


## Exploratory data analysis

:::panel-tabset

### Spam vs. Number char.

```{r}
ggplot(email, aes(x=num_char)) + geom_density(aes(color=spam))
```

### Spam vs. Re: in subject

```{r}
ggplot(email) + geom_bar(aes(x=spam, y=after_stat(prop), group=1)) + facet_wrap(~ re_subj)
```
:::




##

We can fit a logistic regression, as a particular case of a Generalized Linear Model

```{r}
fit= glm(spam ~ num_char + re_subj, data=email, family=binomial())
coefSummary(fit)
```

Both coefficients are negative

- More characters --> lower spam probability

- Containing Re: in the subject --> lower spam probability


## Odds-ratios

Consider individuals $(i,j)$ with the same covariate values, except $x_{i1} \neq x_{j1}$

The odds for an email being spam are
$$
\frac{\pi_i}{1-\pi_i}= e^{x_i^T \beta}; \frac{\pi_j}{1-\pi_j}= e^{x_j^T \beta}
$$

Hence the odds-ratio is
$$
\frac{\pi_i / (1-\pi_i)}{\pi_j/(1-\pi_j)}= e^{x_i^T\beta - x_j^T \beta}=  e^{\beta_1 (x_{i1} - x_{j1})}
$$

- Increase characters by +10: $e^{-0.064 \times 10}= 0.527$

- Re: vs without Re: $e^{-2.97}= 0.051$

## Another example

We seek the odds-ratio associated to +10 characters and adding Re: 
$$
e^{10 \hat{\beta}_1 + \hat{\beta}_2}= e^{-0.064 \times 10 - 2.97}= 0.027
$$

To obtain a 95% CI

- Obtain a CI for $10 \hat{\beta}_1 + \hat{\beta}_2$ with `glht` (seen in linear regression)

- Exponentiate to obtain a CI for $e^{10 \hat{\beta}_1 + \hat{\beta}_2}$

```{r}
C= matrix(c(0,10,1), nrow=1)
ci= as.numeric(confint(glht(fit, C))$confint)
names(ci)= c('estimate','ci.low','ci.up')
round(exp(ci),3)
```


## Plotting the predictions

We use goodies from package `modelr`

```{r}
mygrid= data_grid(email, num_char, re_subj, .model=fit)
fitpred= add_predictions(mygrid, model=fit) #obtain logit email probabilities
fitpred= mutate(fitpred, predprob= 1/(1 + exp(-pred))) #compute email probabilities
ggplot(fitpred, aes(x=num_char,y=predprob,color=re_subj)) + geom_line() + labs(y='Spam probability', x='Number of characters')
```

## Marginal plots

Display effect of 1 covariate, keeping others constant

```{r}
expit= function(x) 1/(1+exp(-x)) #inverse of logit function
fitpred= data_grid(email, num_char, .model=fit) |>
  add_predictions(fit) |>
  mutate(predprob= expit(pred))
fitpred
```

`data_grid` sets factors to their reference category (`re_subj`=0 in this example)


## Marginal plots

```{r}
ggplot(fitpred, aes(num_char, predprob)) + geom_line()
```

## Marginal plots

Better practice to set all other covariates to their mean value.

We can't do this with `data_grid` for factors. Instead, we can do it manually by creating the design matrix

```{r}
x= model.matrix( ~ num_char + re_subj, data=email)
emailnum= data.frame(spam=email$spam, x[,-1]) #x contains intercept, remove
head(emailnum) #now re_subj1 is a numeric vector
```

Then fit the model as usual

```{r}
fitnum= glm(spam ~ num_char + re_subj1, data=emailnum, family=binomial()) 

m= mean(emailnum$re_subj1)
fitpred= data_grid(emailnum, num_char, re_subj1=m, .model=fitnum) |>
  add_predictions(fitnum)
fitpred$predprob= expit(fitpred$pred)
fitpred
```

---

```{r}
ggplot(fitpred, aes(num_char, predprob)) + 
  geom_line() + 
  labs(x="Number of characters", y="Estimated spam probability")
```




## Exercise

Fit a model with an interaction `numchar:re_subj`.
For what value of `re_subj` is the estimated effect of `numchar` stronger?

What's the estimated odds ratio for +10 characters

- For the group where `re_subj` is 0?

- For the group where `re_subj` is 1?

Is the interaction statistically significant (at the usual 0.05 level)?

Plot the estimated spam probability vs. `numchar` and `re_subj`. Does it look similar to the predictions from the model with no interaction? (previous slides)

Turn in an html with your solution [here](https://drive.google.com/drive/folders/1jhh1CrXM3hqc7qTrOYeQzo2pV8Vly1NY?usp=sharing).
Name your file firstname_lastname.html


---

Getting started

```{r}
library(tidyverse)
library(openintro)
source("../code/routines.R")
email
```





# Poisson regression


## Generalized linear models (GLMs)

Generalized linear models are an ample model family based on two components

1. $p(y_i \mid x_i)$ belongs to the so-called exponential family (includes Normal, Binomial, Poisson...)

2. Only its expectation $\mu_i$ depends on $x_i$, and does so via $x_i^T \beta$ (in a 1-to-1 fashion)


**Example.** Logistic regression. $y_i \sim \mbox{Bern}(\mu_i)$
$$
\mu_i = E(y_i \mid x_i)= \frac{1}{1+e^{-x_i^T \beta}}
$$

**Example.** Poisson regression. $y_i \sim \mbox{Poi}(\mu_i)$, with
$$
\mu_i = E(y_i \mid x_i)= e^{x_i^T \beta}
$$

## Poisson model

**Def.** $Y \sim \mbox{Poi}(\mu)$ if
$P(Y=y)= \mu^y  e^{-\mu} / y!$ for $y \in \{0,1,2,\ldots \}$

Moments: $E(Y)= \mu$, $V(Y)= \mu$

**Log-likelihood.** If $y_i \sim \mbox{Poi}(\mu_i)$ where $\mu_i=e^{x_i^T \beta}$ indep $i=1,\ldots,n$,

$$
\begin{aligned}
&\log p(y \mid \beta)= \sum_{i=1}^n \log p(y_i \mid \beta)=
\sum_{i=1}^n \log \left( \frac{e^{x_i^T\beta y_i} e^{-e^{x_i^T \beta}}}{y_i!}  \right) \\
&=\sum_{i=1}^n x_i^T \beta y_i - e^{x_i^T \beta} - \log(y_i!)
\end{aligned}
$$
Log-likelihood is concave in $\beta$ (strictly concave if $X$ has full column rank)


## Parameter interpretation

If two individuals $(i,i')$ are equal in all covariates, except for $x_{i1} \neq x_{i' 1}$, then

$$ \log \mu_i - \log \mu_j= x_{i1} \beta_1 + \sum_{j>1} x_{ij} \beta_j - x_{i'1} \beta_1 - \sum_{j>1} x_{ij} \beta_j= (x_{i1} - x_{i'1}) \beta_1$$
Hence the ratio of their predicted values is
$$
\frac{\mu_i}{\mu_j}= e^{(x_{i1} - x_{i'1}) \beta_1}
$$
If $x_{i1}=1$ and $x_{i'1=0}$, then simply $e^{\beta_1}$


## Philippinnes household survey

<small> Example from the [Beyond MLR book](https://bookdown.org/roback/bookdown-BeyondMLR) </small>

Philippines household survey from 2015 (Philippines Stat. Authority)

- `total`: number of people living in household other than the head (poverty indicator)

- `location`: region where household is located

- `age`: age of household head

- `numLT5`: number in the household <5 years old

- `roof`: type of roof (Light/Salvaged Material, Strong Material)

---

```{r}
fHH= as_tibble(read.table("../datasets/fHH1.txt", header=TRUE))
fHH
```


## Exploratory data analysis

:::panel-tabset

### Barplot

```{r}
ggplot(fHH, aes(x=total)) + geom_bar()
```


### Scatterplot

```{r}
ggplot(fHH, aes(x=age, y=total)) + geom_point()
```

### Jittered scatterplot

```{r}
ggplot(fHH, aes(x=age, y=total)) + geom_point(position="jitter") + geom_smooth()
```

### By region

```{r}
ggplot(fHH, aes(x=age,y=total, color=location)) + geom_point(position="jitter") + geom_smooth()
```

### By roof

```{r}
ggplot(fHH, aes(x=age,y=total, color=roof)) + geom_point(position="jitter") + geom_smooth()
```

:::



## Data pre-processing

Age has a non-linear effect. Two simple options

- Discretize it in (say) 5 year groups

- Add a quadratic effect

```{r}
fHH= mutate(fHH, aged= cut_width(age, 5), age2= age^2)
fHH
```


## Model fit

:::panel-tabset

### Discretized age

```{r}
fit= glm(total ~ location + roof + aged, data=fHH, family=poisson())
summary(fit)$coef
```
### Quadratic age effect

```{r}
fit2= glm(total ~ location + roof + age + age2, data=fHH, family=poisson())
summary(fit2)$coef
```

:::


## Plotting the fit

:::panel-tabset

### aged, location

```{r}
#| code-fold: true
mygrid= data_grid(fHH, aged, location, .model=fit)
fitpred= add_predictions(mygrid, model=fit)
fitpred$predmean= exp(fitpred$pred)
ggplot(fitpred, aes(x=aged,y=predmean,group=location, color=location)) + 
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs(x='Age of household head', y='Expected number of people in household')
```


### aged, roof

```{r}
#| code-fold: true
mygrid= data_grid(fHH, aged, roof, .model=fit)
fitpred= add_predictions(mygrid, model=fit)
fitpred$predmean= exp(fitpred$pred)
ggplot(fitpred, aes(x=aged,y=predmean,group=roof, color=roof)) + 
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs(x='Age of household head', y='Expected number of people in household')
```

### age2, location

```{r}
#| code-fold: true
mygrid= data_grid(fHH, age, location, .model=fit2)
mygrid= mutate(mygrid, age2= age^2)
fitpred= add_predictions(mygrid, model=fit2)
fitpred$predmean= exp(fitpred$pred)

ggplot(fitpred, aes(x=age,y=predmean,group=location,color=location)) + 
  geom_line() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs(x='Age of household head', y='Expected number of people in household') +
  ylim(0,5)
```

### age2, roof

```{r}
#| code-fold: true
mygrid= data_grid(fHH, age, roof, .model=fit2)
mygrid= mutate(mygrid, age2= age^2)
fitpred= add_predictions(mygrid, model=fit2)
fitpred$predmean= exp(fitpred$pred)

ggplot(fitpred, aes(x=age,y=predmean,group=roof,color=roof)) + 
  geom_line() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs(x='Age of household head', y='Expected number of people in household') +
  ylim(0,5)
```

:::



## Exercise

Based on the plot below, an interaction roof vs. age and age$^2$ may be needed. Add them to the model and display the model predictions as a function of age & roof

```{r}
ggplot(fHH, aes(x=age,y=total, color=roof)) + geom_point(position="jitter") + geom_smooth()
```



```{r, eval=FALSE, echo=FALSE}
fit3= glm(total ~ location + roof + age + age2 + age:roof + age2:roof, data=fHH, family=poisson())
coefSummary(fit3)

anova(fit2, fit3, test="Chisq")
```


## A possible solution

```{r, echo=FALSE}
fHHint= mutate(fHH, strong= (roof=='Strong Material')) |>
  mutate(agestrong= age*strong, age2strong= age2*strong)
fit4= glm(total ~ strong + age + age2 + agestrong + age2strong, data=fHHint, family=poisson())

mygrid= data_grid(fHHint, strong, age)
mygrid= mutate(mygrid, age2= age^2, agestrong= age*strong, age2strong= age^2 * strong)
mygrid= add_predictions(mygrid, model=fit4) %>% mutate(predmean= exp(pred))

ggplot(mygrid, aes(x=age,y=predmean,group=strong,color=strong)) + 
  geom_line() +
  labs(x='Age of household head', y='Expected number of people in household', color="Strong roof") +
  ylim(0,5)
```


# Residuals and over-dispersion

## Residuals in GLMs

**Def.** Let $\hat{\mu}_i=\hat{E}(y_i \mid x_i)$. The Pearson residual is
$$
e_i= \frac{y_i - \hat{\mu}_i}{\sqrt{\hat{V}(y_i)}}
$$ 

- Linear regression: $\hat{V}(y_i)= \hat{\sigma}^2$

- Logistic regression: $\hat{V}(y_i)= \hat{\mu}_i (1-\hat{\mu}_i)$

- Poisson regression: $V(y_i)= \hat{\mu}_i$

Alternatively, deviance residuals. They compare the log-likelihood of $y_i$ at $\mu_i=\hat{\mu}_i$ relative to a model with perfect prediction $\mu_i= y_i$

Pearson/deviance residuals are used like residuals in ordinary linear regression


## Example

Take the household occupancy data with a quadratic age effect

```{r}
fHHres= mutate(fHH, pred= predict(fit2), resdev= residuals(fit2, type='deviance'), respearson= residuals(fit2, type='pearson'))
```

:::panel-tabset

### Deviance res.

```{r}
ggplot(fHHres, aes(pred, resdev)) + geom_point() + geom_smooth() + labs(x='Predicted (log)', y='Deviance residual')
```

### Pearson res.

```{r}
ggplot(fHHres, aes(pred, respearson)) + geom_point() + geom_smooth() + labs(x='Predicted (log)', y='Pearson residual')
```

### Constant variance

```{r}
fHHres= mutate(fHHres, predcut= cut_number(pred, 10))
ggplot(fHHres, aes(x=predcut, y=respearson)) + geom_boxplot()
```

:::


## Residual dispersion

Recall that Pearson residuals are
$$
e_i= \frac{y_i - \hat{y}_i}{\sqrt{\hat{V}(y_i)}}
$$

What should the mean & variance of $e_i$ be? Do we observe that in the Philippines data below?

```{r}
mean(fHHres$respearson)
sd(fHHres$respearson)
```



## Over-dispersion

Assumption in logistic/Poisson regression: $\mu_i=E(y_i \mid x_i)$ determines $V(y_i \mid x_i)$

- Logistic: $V(y_i)= \mu_i (1 - \mu_i)$

- Poisson: $V(y_i)= \mu_i$

If the assumed $V(y_i \mid x_i)$ is correct, the variance of Pearson residuals

- Should be $\approx 1$

- Should be constant as $\hat{y}_i$ varies


## Over-dispersion

A popular way to estimate over-dispersion $\phi$.
$$
\hat{\phi}= \frac{\sum_{i=1}^n e_i^2}{n-p}
$$
where $e_i$ are Pearson residuals. If $\hat{\phi}$ much larger than 1, there is over-dispersion

**Result.** If there is over-dispersion, $\hat{\beta}$ has variance $\phi V(\hat{\beta})$, where $V(\hat{\beta})$ is the covariance matrix given by standard theory returned by `glm`

One may then adjust the MLE variance $V(\hat{\beta})$ into 
$$V_Q(\hat{\beta})= \hat{\phi} V(\hat{\beta})$$
where $V_Q$ stands for "quasi-likelihood"


## Example

```{r}
fit2q= glm(total ~ location + roof + age + age2, data=fHH, family=quasipoisson)
```

:::panel-tabset

### No over-dispersion

```{r}
summary(fit2)
```

### Over-dispersion

```{r}
summary(fit2q)
```

:::


## Over-dispersion

Easier to compare visually with `multiplot` from package `coefplot`

```{r}
multiplot(fit2, fit2q)
```

## Bootstrap

Function `bootGLM` at `routines.R` implements the bootstrap for GLMs

```{r}
bootGLM
```

```{r}
mleGLM
```



---

```{r}
f= formula(total ~ location + roof + age + age2)
fit2.bootci= bootGLM(fHH, formula=f, family=poisson(), level=0.95)
```

The bootstrap intervals are also wider than those from MLE theory

```{r}
allci= round(cbind(fit2.bootci[,-1], confint(fit2), confint(fit2q)), 3)
names(allci)= paste(c('Boot','Boot','MLE','MLE','Q','Q'), names(allci), sep='')
allci
```

# Exercise

## Campus crime data

Dataset `campuscrime.txt` from the [Beyond MLR book](https://bookdown.org/roback/bookdown-BeyondMLR) has number of crimes for $n=81$ institutions

- `type`: college (C) or university (U)

- `nv`: number of violent crimes for the institution in 1 year

- `enroll1000`: number of students enrolled at the school (thousands)

- `region`: region of the USA (C = Central, MW = Midwest, NE = Northeast, SE = Southeast, SW = Southwest, and W = West)

Turn in an html with your solution [here](https://drive.google.com/drive/folders/1jhh1CrXM3hqc7qTrOYeQzo2pV8Vly1NY?usp=sharing).
Name your file firstname_lastname.html

---

**Goal.** Study how crimes differ between college and university, after accounting for enrollment and region

- Fit a Poisson regression for crimes vs. `type`, `enroll1000` and `region`. Does `type` have a statistically significant effect at the 0.05 level?

- Provide a point estimate and a 95% interval for the ratio of expected crimes in university / college, that is
$$
\frac{E(y_i \mid x_{i1}=U, x_{i2},\ldots,x_{ip})}{E(y_i \mid x_{i1}=C, x_{i2},\ldots,x_{ip})}
$$

- Is there evidence for over-dispersion? If so, use an over-dispersion adjustment to obtain valid 95% intervals. Do you obtain different conclusions as to whether `type` has a statistically significant effect?



---

Getting started

```{r}
library(tidyverse)
source("../code/routines.R")
crime= as_tibble(read.table("../datasets/campuscrime.txt", header=TRUE, sep="\t"))
crime
```




